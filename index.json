[
{
	"uri": "/calico/install_calico/",
	"title": "Install Calico",
	"tags": [],
	"description": "",
	"content": "Apply the Calico manifest from the aws/amazon-vpc-cni-k8s GitHub project. This creates the daemon sets in the kube-system namespace.\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.2/calico.yaml  Let\u0026rsquo;s go over few key features of the Calico manifest:\n1) We see an annotation throughout; annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.\nkind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: ''* ...  In contrast, Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes the structure of keys and values is constrained, to optimize queries.\n2) We see that the manifest has a tolerations attribute. Taints and tolerations work together to ensure pods are not scheduled onto inappropriate nodes. Taints are applied to nodes, and the only pods that can tolerate the taint are allowed to run on those nodes.\nA taint consists of a key, a value for it and an effect, which can be:\n PreferNoSchedule: Prefer not to schedule intolerant pods to the tainted node NoSchedule: Do not schedule intolerant pods to the tainted node NoExecute: In addition to not scheduling, also evict intolerant pods that are already running on the node.   Like taints, tolerations also have a key value pair and an effect, with the addition of operator. Here in the Calico manifest, we see tolerations has just one attribute: Operator = exists. This means the key value pair is omitted and the toleration will match any taint, ensuring it runs on all nodes.\n tolerations: - operator: Exists  Watch the kube-system daemon sets and wait for the calico-node daemon set to have the DESIRED number of pods in the READY state.\nkubectl get daemonset calico-node --namespace=kube-system  Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  "
},
{
	"uri": "/calico/stars_policy_demo/create_resources/",
	"title": "Create Resources",
	"tags": [],
	"description": "",
	"content": " Before creating network polices, let\u0026rsquo;s create the required resources.\nCreate a new folder for the configuration files.\nmkdir ~/environment/calico_resources cd ~/environment/calico_resources  Stars Namespace Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/namespace.yaml  Let\u0026rsquo;s examine our file by running cat namespace.yaml.\nkind: Namespace apiVersion: v1 metadata: name: stars  Create a namespace called stars:\nkubectl apply -f namespace.yaml  We will create frontend and backend replication controllers and services in this namespace in later steps.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/management-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/backend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/frontend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/client.yaml  cat management-ui.yaml:\napiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001 selector: role: management-ui --- apiVersion: v1 kind: ReplicationController metadata: name: management-ui namespace: management-ui spec: replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001  Create a management-ui namespace, with a management-ui service and replication controller within that namespace:\nkubectl apply -f management-ui.yaml  cat backend.yaml to see how the backend service is built:\napiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: v1 kind: ReplicationController metadata: name: backend namespace: stars spec: replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379  Let\u0026rsquo;s examine the frontend service with cat frontend.yaml:\napiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: v1 kind: ReplicationController metadata: name: frontend namespace: stars spec: replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80  Create frontend and backend replication controllers and services within the stars namespace:\nkubectl apply -f backend.yaml kubectl apply -f frontend.yaml  Lastly, let\u0026rsquo;s examine how the client namespace, and a client service for a replication controller. are built. cat client.yaml:\nkind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: v1 kind: ReplicationController metadata: name: client namespace: client spec: replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client  Apply the client configuraiton.\nkubectl apply -f client.yaml  Check their status, and wait for all the pods to reach the Running status:\n$ kubectl get pods --all-namespaces  Your output should look like this:\nNAMESPACE NAME READY STATUS RESTARTS AGE client client-nkcfg 1/1 Running 0 24m kube-system aws-node-6kqmw 1/1 Running 0 50m kube-system aws-node-grstb 1/1 Running 1 50m kube-system aws-node-m7jg8 1/1 Running 1 50m kube-system calico-node-b5b7j 1/1 Running 0 28m kube-system calico-node-dw694 1/1 Running 0 28m kube-system calico-node-vtz9k 1/1 Running 0 28m kube-system calico-typha-75667d89cb-4q4zx 1/1 Running 0 28m kube-system calico-typha-horizontal-autoscaler-78f747b679-kzzwq 1/1 Running 0 28m kube-system kube-dns-7cc87d595-bd9hq 3/3 Running 0 1h kube-system kube-proxy-lp4vw 1/1 Running 0 50m kube-system kube-proxy-rfljb 1/1 Running 0 50m kube-system kube-proxy-wzlqg 1/1 Running 0 50m management-ui management-ui-wzvz4 1/1 Running 0 24m stars backend-tkjrx 1/1 Running 0 24m stars frontend-q4r84 1/1 Running 0 24m  It may take several minutes to download all the required Docker images.\n To summarize the different resources we created:\n A namespace called stars frontend and backend replication controllers and services within stars namespace A namespace called management-ui Replication controller and service management-ui for the user interface seen on the browser, in the management-ui namespace A namespace called client client replication controller and service in client namespace  "
},
{
	"uri": "/",
	"title": "Amazon EKS Workshop",
	"tags": [],
	"description": "",
	"content": "Amazon EKS Workshop 이 워크샵에서 우리는 VPC, ALB, EC2의 Kubernetes Worker 그리고 Amazon Elastic Container Service for Kubernetes를 구성하는 여러가지 방법을 알아보게 됩니다.\n"
},
{
	"uri": "/prerequisites/self_paced/account/",
	"title": "Create an AWS account",
	"tags": [],
	"description": "",
	"content": "Your account must have the ability to create new IAM roles and scope other IAM permissions.\n  If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n Enter the user details:  Attach the AdministratorAccess IAM Policy:  Click to create the new user:  Take note of the login URL and save:   "
},
{
	"uri": "/conclusion/conclusion/",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\n Deployed an application consisting of microservices Deployed the Kubernetes Dashboard Deployed packages using Helm Deployed a centralized logging infrastructure Configured Automatic scaling of our pods and worker nodes  "
},
{
	"uri": "/calico/stars_policy_demo/default_policy/",
	"title": "Default Pod-to-Pod Communication",
	"tags": [],
	"description": "",
	"content": "In Kubernetes, the pods by default can communicate with other pods, regardless of which host they land on. Every pod gets its own IP address so you do not need to explicitly create links between pods. This is demonstrated by the management-ui.\nkind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001  To open the Management UI, retrieve the DNS name of the Management UI using:\nkubectl get svc -o wide -n management-ui  Copy the EXTERNAL-IP from the output, and paste into a browser. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com” - the full value is the DNS address.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR management-ui LoadBalancer 10.100.239.7 a8b8c5f77eda911e8b1a60ea5d5305a4-720629306.us-east-1.elb.amazonaws.com 80:31919/TCP 9s role=management-ui  The UI here shows the default behavior, of all services being able to reach each other.\n"
},
{
	"uri": "/calico/stars_policy_demo/apply_network_policies/",
	"title": "Apply Network Policies",
	"tags": [],
	"description": "",
	"content": " In a production level cluster, it is not secure to have open pod to pod communication. Let\u0026rsquo;s see how we can isolate the services from each other.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/default-deny.yaml  Let\u0026rsquo;s examine our file by running cat default-deny.yaml.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Let\u0026rsquo;s go over the network policy. Here we see the podSelector does not have any matchLabels, essentially blocking all the pods from accessing it.\nApply the network policy in the stars namespace (frontend and backend services) and the client namespace (client service):\nkubectl apply -n stars -f default-deny.yaml kubectl apply -n client -f default-deny.yaml  Upon refreshing your browser, you see that the management UI cannot reach any of the nodes, so nothing shows up in the UI.\nNetwork policies in Kubernetes use labels to select pods, and define rules on what traffic is allowed to reach those pods. They may specify ingress or egress or both. Each rule allows traffic which matches both the from and ports sections.\nCreate two new network policies.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui-client.yaml  Again, we can examine our file contents by running: cat allow-ui.yaml\nkind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  cat allow-ui-client.yaml\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  Challenge: How do we apply our network policies to allow the traffic we want?\n  Expand here to see the solution   kubectl apply -f allow-ui.yaml kubectl apply -f allow-ui-client.yaml    Upon refreshing your browser, you can see that the management UI can reach all the services, but they cannot communicate with each other.\n"
},
{
	"uri": "/calico/stars_policy_demo/directional_traffic/",
	"title": "Allow Directional Traffic",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s see how we can allow directional traffic from client to frontend and backend.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/backend-policy.yaml wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/frontend-policy.yaml  Let\u0026rsquo;s examine this backend policy with cat backend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST FRONTEND USING PODSELECTOR\u0026gt; ports: - protocol: TCP port: 6379  Challenge: After reviewing the manifest, you\u0026rsquo;ll see we have intentionally left few of the configuration fields for you to EDIT. Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379    Let\u0026rsquo;s examine the frontend policy with cat frontend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST CLIENT USING NAMESPACESELECTOR\u0026gt; ports: - protocol: TCP port: 80  Challenge: Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80    To allow traffic from frontend service to the backend service apply the following manifest:\nkubectl apply -f backend-policy.yaml  And allow traffic from the client namespace to the frontend service:\nkubectl apply -f frontend-policy.yaml  Upon refreshing your browser, you should be able to see the network policies in action:\nLet\u0026rsquo;s have a look at the backend-policy. Its spec has a podSelector that selects all pods with the label role:backend, and allows ingress from all pods that have the label role:frontend and on TCP port 6379, but not the other way round. Traffic is allowed in one direction on a specific port number.\nspec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379  The frontend-policy is similar, except it allows ingress from namespaces that have the label role: client on TCP port 80.\nspec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80  "
},
{
	"uri": "/monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\nhelm ls  Configure Storage Class We will use gp2 EBS volumes for simplicity and demonstration purpose. While deploying in Production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance.\nSave the below manifest as prometheus-storageclass.yaml using your favorite editor.\nChallenge: You need to update provisioner value that is applicable to AWS EBS provisioner. Please see Kubernetes documentation for help\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: \u0026lt;EDIT: UPDATE WITH VALUE OF EBS PROVISIONER\u0026gt; parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Expand here to see the solution   kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Challenge: Create storageclass \u0026ldquo;prometheus\u0026rdquo; by applying proper kubectl command\n  Expand here to see the solution   kubectl create -f prometheus-storageclass.yaml    "
},
{
	"uri": "/deploy/applications/",
	"title": "Deploy our Sample Applications",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  In the sample file above, we describe the service and how it should be deployed. We will write this description to the kubernetes api using kubectl, and kubernetes will ensure our preferences are met as the application is deployed.\nThe containers listen on port 3000, and native service discovery will be used to locate the running containers and communicate with them.\n"
},
{
	"uri": "/calico/stars_policy_demo/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up the demo by deleting the namespaces:\nkubectl delete ns client stars management-ui  "
},
{
	"uri": "/statefulset/storageclass/",
	"title": "Define Storageclass",
	"tags": [],
	"description": "",
	"content": " Introduction Dynamic Volume Provisioning allows storage volumes to be created on-demand. StorageClass should be pre-created to define which provisoner should be used and what parameters should be passed when dynamic provisioning is invoked. (See parameters for AWS EBS)\nDefine Storage Class Copy/Paste the following commands into your Cloud9 Terminal.\nmkdir ~/environment/templates cd ~/environment/templates wget https://eksworkshop.com/statefulset/storageclass.files/mysql-storageclass.yml  Check the configuration of mysql-storageclass.yml file by following command.\ncat ~/environment/templates/mysql-storageclass.yml  You can see provisioner is kubernetes.io/aws-ebs and type is gp2 specified as a parameter.\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: mysql-gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Delete mountOptions: - debug  Create storageclass \u0026ldquo;mysql-gp2\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-storageclass.yml  We will specify \u0026ldquo;mysql-gp2\u0026rdquo; as the storageClassName in volumeClaimTemplates at \u0026ldquo;Create StatefulSet\u0026rdquo; section later.\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] storageClassName: mysql-gp2 resources: requests: storage: 10Gi    Related files   mysql-storageclass.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_intro/install/",
	"title": "Install Helm CLI",
	"tags": [],
	"description": "",
	"content": " Before we can get started configuring helm we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh  Once you install helm, the command will prompt you to run \u0026lsquo;helm init\u0026rsquo;. Do not run \u0026lsquo;helm init\u0026rsquo;. Follow the instructions to configure helm using Kubernetes RBAC and then install tiller as specified below If you accidentally run \u0026lsquo;helm init\u0026rsquo;, you can safely uninstall tiller by running \u0026lsquo;helm reset \u0026ndash;force\u0026rsquo;\n Configure Helm access with RBAC Helm relies on a service called tiller that requires special permission on the kubernetes cluster, so we need to build a Service Account for tiller to use. We\u0026rsquo;ll then apply this to the cluster.\nTo create a new service account manifest:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF  Next apply the config:\nkubectl apply -f ~/environment/rbac.yaml  Then we can install helm using the helm tooling\nhelm init --service-account tiller  This will install tiller into the cluster which gives it access to manage resources in your cluster.\n"
},
{
	"uri": "/healthchecks/livenessprobe/",
	"title": "Configure Liveness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Use the command below to create a directory\nmkdir -p ~/environment/healthchecks  Save the manifest as ~/environment/healthchecks/liveness-app.yaml using your favorite editor. You can review the manifest that is described below. In the configuration file, livenessProbe determines how kubelet should check the Container in order to consider whether it is healthy or not. kubelet uses periodSeconds field to do frequent check on the Container. In this case, kubelet checks liveness probe every 5 seconds. initialDelaySeconds field is to tell the kubelet that it should wait for 5 seconds before doing the first probe. To perform a probe, kubelet sends a HTTP GET request to the server hosting this Pod and if the handler for the servers /health returns a success code, then the Container is considered healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.\napiVersion: v1 kind: Pod metadata: name: liveness-app spec: containers: - name: liveness image: brentley/ecsdemo-nodejs livenessProbe: httpGet: path: /health port: 3000 initialDelaySeconds: 5 periodSeconds: 5  Let\u0026rsquo;s create the pod using the manifest\nkubectl apply -f ~/environment/healthchecks/liveness-app.yaml  The above command creates a pod with liveness probe\nkubectl get pod liveness-app  The output looks like below. Notice the RESTARTS\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 0 11s  The kubectl describe command will show an event history which will show any probe failures or restarts.\nkubectl describe pod liveness-app  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 38s kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Normal Pulling 37s kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 37s kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 37s kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 37s kubelet, ip-192-168-18-63.ec2.internal Started container  Introduce a Failure We will run the next command to send a SIGUSR1 signal to the nodejs application. By issuing this command we will send a kill signal to the application process in docker runtime.\nkubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1  Describe the pod after waiting for 15-20 seconds and you will notice kubelet actions of killing the Container and restarting it.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 1m kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Warning Unhealthy 30s (x3 over 40s) kubelet, ip-192-168-18-63.ec2.internal Liveness probe failed: Get http://192.168.13.176:3000/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Normal Pulling 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Started container Normal Killing 0s kubelet, ip-192-168-18-63.ec2.internal Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.  When the nodejs application entered a debug mode with SIGUSR1 signal, it did not respond to the health check pings and kubelet killed the container. The container was subject to the default restart policy.\nkubectl get pod liveness-app  The output looks like below\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 1 12m  Challenge: How can we check the status of the container health checks?\n  Expand here to see the solution   kubectl logs liveness-app  You can also use kubectl logs to retrieve logs from a previous instantiation of a container with --previous flag, in case the container has crashed\nkubectl logs liveness-app --previous  \u0026lt;Output omitted\u0026gt; Example app listening on port 3000! ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:01 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 16 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:06 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:11 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; Starting debugger agent. Debugger listening on [::]:5858    "
},
{
	"uri": "/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction to Kubernetes A walkthrough of basic Kubernetes concepts.\nWelcome to the Amazon EKS Workshop!\nThe intent of this workshop is to educate users about the features of Amazon EKS.\nBackground in EKS, Kubernetes, Docker, and container workflows are not required, but they are recommended.\nThis chapter will introduce you to the basic workings of Kubernetes, laying the foundation for the hands-on portion of the workshop.\nSpecifically, we will walk you through the following topics:\n Kubernetes (k8s) Basics   Kubernetes Architecture   Amazon EKS   "
},
{
	"uri": "/batch/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction Batch processing refers to performing units of work, referred to as a job in a repetitive and unattended fashion. Jobs are typically grouped together and processed in batches (hence the name).\nKubernetes includes native support for running Jobs. Jobs can run multiple pods in parallel until receiving a set number of completions. Each pod can contain multiple containers as a single unit of work.\nArgo enhances the batch processing experience by introducing a number of features:\n Steps based declaration of workflows Artifact support Step level inputs \u0026amp; outputs Loops Conditionals Visualization (using Argo Dashboard) \u0026hellip;and more  In this module, we will build a simple Kubernetes Job, recreate that job in Argo, and add common features and workflows for more advanced batch processing.\n"
},
{
	"uri": "/servicemesh_with_appmesh/components/",
	"title": "Components",
	"tags": [],
	"description": "",
	"content": "App Mesh is made up of the following components:\n Service mesh: A service mesh is a logical boundary for network traffic between the services that reside within it. For more information, see Service Meshes.\n Virtual nodes: A virtual node acts as a logical pointer to a particular task group, such as an ECS service or a Kubernetes deployment. When you create a virtual node, you must specify the DNS service discovery name for your task group. For more information, see Virtual Nodes.\n Envoy proxy and router manager: The Envoy proxy and its router manager container images configure your microservice task group to use the App Mesh service mesh traffic rules that you set up for your virtual routers and virtual nodes. You add these containers to your task group after you have created your virtual nodes, virtual routers, and routes. For more information, see Envoy and Proxy Route Manager Images.\n Virtual routers: The virtual router handles traffic for one or more service names within your mesh. For more information, see Virtual Routers.\n Routes: A route is associated with a virtual router, and it directs traffic that matches a service name prefix to one or more virtual nodes. For more information, see Routes.\n  In this chapter, we\u0026rsquo;ll set up these components, and deploy a simple microservice to it, and then modify the App Mesh routes to demonstrate a canary deployment.\n"
},
{
	"uri": "/servicemesh_with_istio/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Istio Istio is a completely open source service mesh that layers transparently onto existing distributed applications. It\u0026rsquo;s also a platform, including APIs, that let it integrate into any logging platform, or telemetry or policy system.\nLet\u0026rsquo;s review in more detail what each of the components that make up this service mesh are.\n Envoy\n Processes the inbound/outbound traffic from inter-service and service-to-external-service transparently.  Pilot\n Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.)  Mixer\n Mixer enforces access control and usage policies across the service mesh, and collects telemetry data from the Envoy proxy and other services.  Citadel\n Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management.   "
},
{
	"uri": "/monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": " Download Prometheus curl -o prometheus-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/prometheus/values.yaml  Open the prometheus-values.yaml you downloaded by double clicking on the file name on the left panel. You need to make three edits to this file.\nSearch for storageClass in the prometheus-values.yaml, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. You will do this twice, under both server \u0026amp; alertmanager manifests\nThe third edit you will do is to expose Prometheus server as a NodePort. Because Prometheus is exposed as ClusterIP by default, the web UI cannot be reached outside of Kubernetes. By exposing the service as NodePort, we will be able to reach Prometheus web UI from the worker node IP address. Search for type: ClusterIP and add nodePort: 30900 and change the type to NodePort as indicated below.\nThis configuration is not recommended in Production and there are better ways to secure it. You can read more about exposing Prometheus web UI in this link\nWhen you search, you will find there are more than one type: ClusterIP in prometheus-values.yaml. You need to update relevant Prometheus manifest. See below snippet for identifying Prometheus manifest\n ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort    Expand here to see the complete yaml   rbac: create: true ## Define serviceAccount names for components. Defaults to component's fully qualified name. ## serviceAccounts: alertmanager: create: true name: kubeStateMetrics: create: true name: nodeExporter: create: true name: pushgateway: create: true name: server: create: true name: alertmanager: ## If false, alertmanager will not be installed ## enabled: true ## alertmanager container name ## name: alertmanager ## alertmanager container image ## image: repository: prom/alertmanager tag: v0.15.2 pullPolicy: IfNotPresent ## alertmanager priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional alertmanager container arguments ## extraArgs: {} ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;/\u0026quot; ## Additional alertmanager container environment variable ## For instance to add a http_proxy ## extraEnv: {} ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, alertmanager Ingress will be created ## enabled: false ## alertmanager Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## alertmanager Ingress additional labels ## extraLabels: {} ## alertmanager Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - alertmanager.domain.com # - domain.com/alertmanager ## alertmanager Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - alertmanager.domain.com ## Alertmanager Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for alertmanager scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for alertmanager pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, alertmanager will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## alertmanager data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## alertmanager data Persistent Volume Claim annotations ## annotations: {} ## alertmanager data Persistent Volume existing claim name ## Requires alertmanager.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## alertmanager data Persistent Volume mount root path ## mountPath: /data ## alertmanager data Persistent Volume size ## size: 2Gi ## alertmanager data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of alertmanager data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to alertmanager pods ## podAnnotations: {} replicaCount: 1 ## alertmanager resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to alertmanager pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## Enabling peer mesh service end points for enabling the HA alert manager ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md # enableMeshPeer : true ## List of IP addresses at which the alertmanager service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 # nodePort: 30000 type: ClusterIP ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.2.2 pullPolicy: IfNotPresent ## Additional configmap-reload container arguments ## extraArgs: {} ## Additional configmap-reload mounts ## extraConfigmapMounts: [] # - name: prometheus-alerts # mountPath: /etc/alerts.d # subPath: \u0026quot;\u0026quot; # configMap: prometheus-alerts # readOnly: true ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} initChownData: ## If false, data ownership will not be reset at startup ## This allows the prometheus-server to be run with an arbitrary user ## enabled: true ## initChownData container name ## name: init-chown-data ## initChownData container image ## image: repository: busybox tag: latest pullPolicy: IfNotPresent ## initChownData resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} kubeStateMetrics: ## If false, kube-state-metrics will not be installed ## enabled: true ## kube-state-metrics container name ## name: kube-state-metrics ## kube-state-metrics container image ## image: repository: quay.io/coreos/kube-state-metrics tag: v1.4.0 pullPolicy: IfNotPresent ## kube-state-metrics priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## kube-state-metrics container arguments ## args: {} ## Node tolerations for kube-state-metrics scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for kube-state-metrics pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to kube-state-metrics pods ## podAnnotations: {} pod: labels: {} replicaCount: 1 ## kube-state-metrics resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 16Mi # requests: # cpu: 10m # memory: 16Mi ## Security context to be added to kube-state-metrics pods ## securityContext: {} service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the kube-state-metrics service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 type: ClusterIP nodeExporter: ## If false, node-exporter will not be installed ## enabled: true ## If true, node-exporter pods share the host network namespace ## hostNetwork: true ## If true, node-exporter pods share the host PID namespace ## hostPID: true ## node-exporter container name ## name: node-exporter ## node-exporter container image ## image: repository: prom/node-exporter tag: v0.16.0 pullPolicy: IfNotPresent ## Specify if a Pod Security Policy for node-exporter must be created ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: enabled: False ## node-exporter priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Custom Update Strategy ## updateStrategy: type: OnDelete ## Additional node-exporter container arguments ## extraArgs: {} ## Additional node-exporter hostPath mounts ## extraHostPathMounts: [] # - name: textfile-dir # mountPath: /srv/txt_collector # hostPath: /var/lib/node-exporter # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # configMap: certs-configmap # readOnly: true ## Node tolerations for node-exporter scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for node-exporter pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to node-exporter pods ## podAnnotations: {} ## Labels to be added to node-exporter pods ## pod: labels: {} ## node-exporter resource limits \u0026amp; requests ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 200m # memory: 50Mi # requests: # cpu: 100m # memory: 30Mi ## Security context to be added to node-exporter pods ## securityContext: {} # runAsUser: 0 service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the node-exporter service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] hostPort: 9100 loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9100 type: ClusterIP server: ## Prometheus server container name ## name: server ## Prometheus server container image ## image: repository: prom/prometheus tag: v2.4.3 pullPolicy: IfNotPresent ## prometheus server priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;\u0026quot; ## This flag controls access to the administrative HTTP API which includes functionality such as deleting time ## series. This is disabled by default. enableAdminApi: false global: ## How frequently to scrape targets by default ## scrape_interval: 1m ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 1m ## Additional Prometheus server container arguments ## extraArgs: {} ## Additional Prometheus server hostPath mounts ## extraHostPathMounts: [] # - name: certs-dir # mountPath: /etc/kubernetes/certs # subPath: \u0026quot;\u0026quot; # hostPath: /etc/kubernetes/certs # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # subPath: \u0026quot;\u0026quot; # configMap: certs-configmap # readOnly: true ## Additional Prometheus server Secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # subPath: \u0026quot;\u0026quot; # secretName: prom-secret-files # readOnly: true ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/server-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, Prometheus server Ingress will be created ## enabled: false ## Prometheus server Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## Prometheus server Ingress additional labels ## extraLabels: {} ## Prometheus server Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - prometheus.domain.com # - domain.com/prometheus ## Prometheus server Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-server-tls # hosts: # - prometheus.domain.com ## Server Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for server scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for Prometheus server pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 8Gi ## Prometheus server data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of Prometheus server data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to Prometheus server pods ## podAnnotations: {} # iam.amazonaws.com/role: prometheus replicaCount: 1 ## Prometheus server resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 500m # memory: 512Mi # requests: # cpu: 500m # memory: 512Mi ## Security context to be added to server pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [54.210.142.247] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort ## Prometheus server pod termination grace period ## terminationGracePeriodSeconds: 300 ## Prometheus data retention period (i.e 360h) ## retention: \u0026quot;\u0026quot; pushgateway: ## If false, pushgateway will not be installed ## enabled: true ## pushgateway container name ## name: pushgateway ## pushgateway container image ## image: repository: prom/pushgateway tag: v0.5.2 pullPolicy: IfNotPresent ## pushgateway priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional pushgateway container arguments ## extraArgs: {} ingress: ## If true, pushgateway Ingress will be created ## enabled: false ## pushgateway Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## pushgateway Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - pushgateway.domain.com # - domain.com/pushgateway ## pushgateway Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - pushgateway.domain.com ## Node tolerations for pushgateway scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for pushgateway pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to pushgateway pods ## podAnnotations: {} replicaCount: 1 ## pushgateway resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to push-gateway pods ## securityContext: {} service: annotations: prometheus.io/probe: pushgateway labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the pushgateway service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9091 type: ClusterIP ## alertmanager ConfigMap entries ## alertmanagerFiles: alertmanager.yml: global: {} # slack_api_url: '' receivers: - name: default-receiver # slack_configs: # - channel: '@you' # send_resolved: true route: group_wait: 10s group_interval: 5m receiver: default-receiver repeat_interval: 3h ## Prometheus server ConfigMap entries ## serverFiles: alerts: {} rules: {} prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # A scrape configuration for running Prometheus on a Kubernetes cluster. # This uses separate scrape configs for cluster components (i.e. API server, node) # and services to allow each to use different authentication configs. # # Kubernetes labels will be added as Prometheus labels on metrics via the # `labelmap` relabeling action. # Scrape config for API servers. # # Kubernetes exposes API servers as endpoints to the default/kubernetes # service so this uses `endpoints` role and uses relabelling to only keep # the endpoints associated with the default/kubernetes service using the # default named port `https`. This works for single API server deployments as # well as HA API server deployments. - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token # Keep only the default/kubernetes service endpoints for the https port. This # will add targets for each API server which Kubernetes adds an endpoint to # the default/kubernetes service. relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-nodes-cadvisor' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node # This configuration will work only on kubelet 1.7.3+ # As the scrape endpoints for cAdvisor have changed # if you are using older version you need to change the replacement to # replacement: /api/v1/nodes/${1}:4194/proxy/metrics # more info here https://github.com/coreos/prometheus-operator/issues/633 relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'prometheus-pushgateway' honor_labels: true kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: pushgateway # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # Example scrape config for pods # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true` # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # adds additional scrape configs to prometheus.yml # must be a string so you have to add a | after extraScrapeConfigs: # example adds prometheus-blackbox-exporter scrape config extraScrapeConfigs: # - job_name: 'prometheus-blackbox-exporter' # metrics_path: /probe # params: # module: [http_2xx] # static_configs: # - targets: # - https://example.com # relabel_configs: # - source_labels: [__address__] # target_label: __param_target # - source_labels: [__param_target] # target_label: instance # - target_label: __address__ # replacement: prometheus-blackbox-exporter:9115 networkPolicy: ## Enable creation of NetworkPolicy resources. ## enabled: false    Deploy Prometheus helm install -f prometheus-values.yaml stable/prometheus --name prometheus --namespace prometheus  Make a note of prometheus endpoint in helm response (you will need this later). It should look similar to below\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local  Check if Prometheus components deployed as expected\nkubectl get all -n prometheus  You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-77cfdf85db-s9p48 2/2 Running 0 1m pod/prometheus-kube-state-metrics-74d5c694c7-vqtjd 1/1 Running 0 1m pod/prometheus-node-exporter-6dhpw 1/1 Running 0 1m pod/prometheus-node-exporter-nrfkn 1/1 Running 0 1m pod/prometheus-node-exporter-rtrm8 1/1 Running 0 1m pod/prometheus-pushgateway-d5fdc4f5b-dbmrg 1/1 Running 0 1m pod/prometheus-server-6d665b876-dsmh9 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.89.154 \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 1m service/prometheus-pushgateway ClusterIP 10.100.136.143 \u0026lt;none\u0026gt; 9091/TCP 1m service/prometheus-server NodePort 10.100.151.245 \u0026lt;none\u0026gt; 80/30900 1m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1 1 1 1 1m deployment.apps/prometheus-kube-state-metrics 1 1 1 1 1m deployment.apps/prometheus-pushgateway 1 1 1 1 1m deployment.apps/prometheus-server 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-77cfdf85db 1 1 1 1m replicaset.apps/prometheus-kube-state-metrics-74d5c694c7 1 1 1 1m replicaset.apps/prometheus-pushgateway-d5fdc4f5b 1 1 1 1m replicaset.apps/prometheus-server-6d665b876 1 1 1 1m  You can access Prometheus server URL by going to any one of your Worker node IP address and specify port :30900/targets (for ex, 52.12.161.128:30900/targets. Remember to open port 30900 in your Worker nodes Security Group. In the web UI, you can see all the targets and metrics being monitored by Prometheus\n"
},
{
	"uri": "/deploy/deploynodejs/",
	"title": "Deploy NodeJS Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the NodeJS Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-nodejs  "
},
{
	"uri": "/eksctl/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this module, we need to download the eksctl binary:\ncurl --silent --location \u0026quot;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/eksctl /usr/local/bin  Confirm the eksctl command works:\neksctl version  "
},
{
	"uri": "/spotworkers/workers/",
	"title": "Add EC2 Workers - On-Demand and Spot",
	"tags": [],
	"description": "",
	"content": " We have our EKS Cluster and worker nodes already, but we need some Spot Instances configured as workers. We also need a Node Labeling strategy to identify which instances are Spot and which are on-demand so that we can make more intelligent scheduling decisions. We will use AWS CloudFormation to launch new worker nodes that will connect to the EKS cluster.\nThis template will create a single ASG that leverages the latest feature to mix multiple instance types and purchase as a single K8s nodegroup. Check out this blog: New – EC2 Auto Scaling Groups With Multiple Instance Types \u0026amp; Purchase Options for details.\nRetrieve the Worker Role name First, we will need to collect the Role Name that is in use with our EKS worker nodes\necho $ROLE_NAME  Copy the Role Name for use as a Parameter in the next step. If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Role Name   INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile    # Example Output eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXXXXX  Retrieve the Security Group Name We also need to collect the ID of the security group used with the existing worker nodes.\nSTACK_NAME=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) SG_ID=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME --logical-resource-id SG | jq -r '.StackResources[].PhysicalResourceId') echo $SG_ID  # Example Output sg-0d9fb7e709dff5675  Launch the CloudFormation Stack We will launch the CloudFormation template as a new set of worker nodes, but it\u0026rsquo;s also possible to update the nodegroup CloudFormation stack created by the eksctl tool.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       EKS Workers - Spot and On Demand  Launch    Download      Confirm the region is correct based on where you\u0026rsquo;ve deployed your cluster.\n Once the console is open you will need to configure the missing parameters. Use the table below for guidance.\n   Parameter Value     Stack Name: eksworkshop-spot-workers   Cluster Name: eksworkshop-eksctl (or whatever you named your cluster)   ClusterControlPlaneSecurityGroup: Select from the dropdown. It will contain your cluster name and the words \u0026lsquo;ControlPlaneSecurityGroup\u0026rsquo;   NodeInstanceRole: Use the role name that copied in the step above. (e.g. eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXX)   UseExistingNodeSecurityGroups: Leave as \u0026lsquo;Yes\u0026rsquo;   ExistingNodeSecurityGroups: Use the SG name that copied in the step above. (e.g. sg-0123456789abcdef)   NodeImageId: Visit this link and select the non-GPU image for your region - Check for empty spaces in copy/paste   KeyName: SSH Key Pair created earlier or any valid key will work   NodeGroupName: Leave as spotworkers   VpcId: Select your workshop VPC from the dropdown   Subnets: Select the 3 private subnets for your workshop VPC from the dropdown   BootstrapArgumentsForOnDemand: --kubelet-extra-args --node-labels=lifecycle=OnDemand   BootstrapArgumentsForSpotFleet: --kubelet-extra-args '--node-labels=lifecycle=Ec2Spot --register-with-taints=spotInstance=true:PreferNoSchedule'    What\u0026rsquo;s going on with Bootstrap Arguments? The EKS Bootstrap.sh script is packaged into the EKS Optimized AMI that we are using, and only requires a single input, the EKS Cluster name. The bootstrap script supports setting any kubelet-extra-args at runtime. We have configured node-labels so that kubernetes knows what type of nodes we have provisioned. We set the lifecycle for the nodes as OnDemand or Ec2Spot. We are also tainting with PreferNoSchedule to prefer pods not be scheduled on Spot Instances. This is a “preference” or “soft” version of NoSchedule – the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required.\nYou can leave the rest of the default parameters as is and continue through the remaining CloudFormation screens. Check the box next to I acknowledge that AWS CloudFormation might create IAM resources and click Create\nThe creation of the workers will take about 3 minutes.\n Confirm the Nodes Confirm that the new nodes joined the cluster correctly. You should see 2-3 more nodes added to the cluster.\nkubectl get nodes  You can use the node-labels to identify the lifecycle of the nodes\nkubectl get nodes --show-labels --selector=lifecycle=Ec2Spot  The output of this command should return 2 nodes. At the end of the node output, you should see the node label lifecycle=Ec2Spot\nNow we will show all nodes with the lifecycle=OnDemand. The output of this command should return 1 node as configured in our CloudFormation template.\nkubectl get nodes --show-labels --selector=lifecycle=OnDemand  You can use the kubectl describe nodes with one of the spot nodes to see the taints applied to the EC2 Spot Instances.\n"
},
{
	"uri": "/scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": " Deploy the Metrics Server Metrics Server is a cluster-wide aggregator of resource usage data. These metrics will drive the scaling behavior of the deployments. We will deploy the metrics server using Helm configured in a previous module\nhelm install stable/metrics-server \\ --name metrics-server \\ --version 2.0.4 \\ --namespace metrics  Confirm the Metrics API is available. Return to the terminal in the Cloud9 Environment\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml  If all is well, you should see a status message similar to the one below in the response\nstatus: conditions: - lastTransitionTime: 2018-10-15T15:13:13Z message: all checks passed reason: Passed status: \u0026quot;True\u0026quot; type: Available  We are now ready to scale a deployed application "
},
{
	"uri": "/logging/prereqs/",
	"title": "Configure IAM Policy for Worker Nodes",
	"tags": [],
	"description": "",
	"content": "We will be deploying Fluentd as a DaemonSet, or one pod per worker node. The fluentd log daemon will collect logs and forward to CloudWatch Logs. This will require the nodes to have permissions to send logs and create log groups and log streams. This can be accomplished with an IAM user, IAM role, or by using a tool like Kube2IAM.\nIn our example, we will create an IAM policy and attach it the the Worker node role.\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  Create a new IAM Policy and attach it to the Worker Node Role.\nmkdir ~/environment/iam_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/k8s-logs-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/environment/iam_policy/k8s-logs-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker  "
},
{
	"uri": "/statefulset/configmap/",
	"title": "Create ConfigMap",
	"tags": [],
	"description": "",
	"content": " Introduction ConfigMap allow you to decouple configuration artifacts and secrets from image content to keep containerized applications portable. Using ConfigMap, you can independently control MySQL configuration.\nCreate ConfigMap Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/configmap.files/mysql-configmap.yml  Check the configuration of mysql-configmap.yml file by following command.\ncat ~/environment/templates/mysql-configmap.yml  ConfigMap stores master.cnf, slave.cnf and pass them when initializing master and slave pods defined in statefulset. master.cnf is for the MySQL master pod which has binary log option (log-bin) to provides a record of the data changes to be sent to slave servers and slave.cnf is for slave pods which has super-read-only option.\napiVersion: v1 kind: ConfigMap metadata: name: mysql-config labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only  Create configmap \u0026ldquo;mysql-config\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-configmap.yml    Related files   mysql-configmap.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_micro/create_chart/",
	"title": "Create a Chart",
	"tags": [],
	"description": "",
	"content": "Helm charts have a structure similar to:\n/eksdemo /Chart.yaml # a description of the chart /values.yaml # defaults, may be overridden during install or upgrade /charts/ # May contain subcharts /templates/ # the template files themselves ...  We\u0026rsquo;ll follow this template, and create a new chart called eksdemo with the following commands:\ncd ~/environment helm create eksdemo  "
},
{
	"uri": "/dashboard/dashboard/",
	"title": "Deploy the Official Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": "The official Kubernetes dashboard is not deployed by default, but there are instructions in the official documentation\nWe can deploy the dashboard with the following command:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  Since this is deployed to our private cluster, we need to access it via a proxy. Kube-proxy is available to proxy our requests to the dashboard service. In your workspace, run the following command:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n "
},
{
	"uri": "/healthchecks/readinessprobe/",
	"title": "Configure Readiness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Save the text from following block as ~/environment/healthchecks/readiness-deployment.yaml. The readinessProbe definition explains how a linux command can be configured as healthcheck. We create an empty file /tmp/healthy to configure readiness probe and use the same to understand how kubelet helps to update a deployment with only healthy pods.\napiVersion: apps/v1 kind: Deployment metadata: name: readiness-deployment spec: replicas: 3 selector: matchLabels: app: readiness-deployment template: metadata: labels: app: readiness-deployment spec: containers: - name: readiness-deployment image: alpine command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;touch /tmp/healthy \u0026amp;\u0026amp; sleep 86400\u0026quot;] readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 3  We will now create a deployment to test readiness probe\nkubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml  The above command creates a deployment with 3 replicas and readiness probe as described in the beginning\nkubectl get pods -l app=readiness-deployment  The output looks similar to below\n NAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 1/1 Running 0 31s readiness-deployment-7869b5d679-vd55d 1/1 Running 0 31s readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 31s  Let us also confirm that all the replicas are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below\nReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable  Introduce a Failure Pick one of the pods from above 3 and issue a command as below to delete the /tmp/healthy file which makes the readiness probe fail.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- rm /tmp/healthy  readiness-deployment-7869b5d679-922mx was picked in our example cluster. The /tmp/healthy file was deleted. This file must be present for the readiness check to pass. Below is the status after issuing the command.\nkubectl get pods -l app=readiness-deployment  The output looks similar to below:\nNAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 0/1 Running 0 4m readiness-deployment-7869b5d679-vd55d 1/1 Running 0 4m readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 4m  Traffic will not be routed to the first pod in the above deployment. The ready column confirms that the readiness probe for this pod did not pass and hence was marked as not ready.\nWe will now check for the replicas that are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below\nReplicas: 3 desired | 3 updated | 3 total | 2 available | 1 unavailable  When the readiness probe for a pod fails, the endpoints controller removes the pod from list of endpoints of all services that match the pod.\nChallenge: How would you restore the pod to Ready status?   Expand here to see the solution   Run the below command with the name of the pod to recreate the /tmp/healthy file. Once the pod passes the probe, it gets marked as ready and will begin to receive traffic again.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- touch /tmp/healthy  kubectl get pods -l app=readiness-deployment   \n"
},
{
	"uri": "/codepipeline/role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a sample Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the role:\ncd ~/environment ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::${ACCOUNT_ID}:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuildKubectlRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy  "
},
{
	"uri": "/prerequisites/self_paced/workspace/",
	"title": "Create a Workspace",
	"tags": [],
	"description": "",
	"content": " The Cloud9 workspace should be built by an IAM user with Administrator privileges, not the root account user. Please ensure you are logged in as an IAM user, not the root account user.\n This workshop was designed to run in the Oregon (us-west-2) region. Please don\u0026rsquo;t run in any other region. Future versions of this workshop will expand region availability, and this message will be removed.\n -- Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n Launch Cloud9 in your closest region:  Oregon Ohio Ireland  Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n  $(function(){$(\"#region\").tabs();});  Select Create environment Name it eksworkshop, and take all other defaults When it comes up, customize the environment by closing the welcome tab and lower work area, and opening a new terminal tab in the main work area:  Your workspace should now look like this:  If you like this theme, you can choose it yourself by selecting View / Themes / Solarized / Solarized Dark in the Cloud9 workspace menu.\n  "
},
{
	"uri": "/x-ray/role/",
	"title": "Modify IAM Role",
	"tags": [],
	"description": "",
	"content": "In order for the X-Ray daemon to communicate with the service, we need to add a policy to the worker nodes\u0026rsquo; AWS Identity and Access Management (IAM) role.\nModify the role in the Cloud9 terminal:\n  Expand here if you need to export the Role Name   INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile    aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess  "
},
{
	"uri": "/prerequisites/",
	"title": "Start the workshop...",
	"tags": [],
	"description": "",
	"content": " Getting Started  Create an AWS account   Create a Workspace   Create an SSH key   Install Kubernetes Tools   Clone the Service Repos   Create an IAM role for your Workspace   Attach the IAM role to your Workspace   Update IAM settings for your Workspace   "
},
{
	"uri": "/advanced-networking/secondary_cidr/",
	"title": "Using Secondary CIDRs with EKS",
	"tags": [],
	"description": "",
	"content": " Using Secondary CIDRs with EKS You can expand your VPC network by adding additional CIDR ranges. This capability can be used if you are running out of IP ranges within your existing VPC or if you have consumed all available RFC 1918 CIDR ranges within your corporate network. EKS supports additional IPv4 CIDR blocks in the 100.64.0.0/10 and 198.19.0.0/16 ranges. You can review this announcement from our what\u0026rsquo;s new blog\nIn this tutorial, we will walk you through the configuration that is needed so that you can launch your Pod networking on top of secondary CIDRs\n"
},
{
	"uri": "/x-ray/x-ray-daemon/",
	"title": "Deploy X-Ray DaemonSet",
	"tags": [],
	"description": "",
	"content": "Now that we have modified the IAM role for the worker nodes to permit write operations to the X-Ray service, we are going to deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster. For reference, see the example implementation used in this module.\nThe AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point to xray-service.default:2000.\nThe following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.\nfunc init() { xray.Configure(xray.Config{ DaemonAddr: \u0026quot;xray-service.default:2000\u0026quot;, LogLevel: \u0026quot;info\u0026quot;, }) }  To deploy the X-Ray DaemonSet:\nkubectl create -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  To see the status of the X-Ray DaemonSet:\nkubectl describe daemonset xray-daemon  The folllowing is an example of the command output:\nTo view the logs for all of the X-Ray daemon pods run the following\n kubectl logs -l app=xray-daemon  "
},
{
	"uri": "/codepipeline/configmap/",
	"title": "Modify aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster.\nOnce the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE=\u0026quot; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksWorkshopCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  If you would like to edit the aws-auth ConfigMap manually, you can run: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/x-ray/microservices/",
	"title": "Deploy Example Microservices",
	"tags": [],
	"description": "",
	"content": "We now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.\nIn this step, we are going to deploy example front-end and back-end microservices to the cluster. The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby, .NET and Java.\nkubectl apply -f https://eksworkshop.com/x-ray/sample-front.files/x-ray-sample-front-k8s.yml kubectl apply -f https://eksworkshop.com/x-ray/sample-back.files/x-ray-sample-back-k8s.yml  To review the status of the deployments, you can run:\nkubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8s  For the status of the services, run the following command:\nkubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8s  Once the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get service x-ray-sample-front-k8s -o wide  After your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service. The JSON document displayed in the browser is the result of the request made to the back-end service.\nThis service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated.\n "
},
{
	"uri": "/codepipeline/forksample/",
	"title": "Fork Sample Repository",
	"tags": [],
	"description": "",
	"content": "We are now going to fork the sample Kubernetes service so that we will be able modify the repository and trigger builds.\nLogin to GitHub and fork the sample service to your own account:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nOnce the repo is forked, you can view it in your your GitHub repositories.\nThe forked repo will look like:\n"
},
{
	"uri": "/codepipeline/githubcredentials/",
	"title": "GitHub Access Token",
	"tags": [],
	"description": "",
	"content": "In order for CodePipeline to receive callbacks from GitHub, we need to generate a personal access token.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\n"
},
{
	"uri": "/x-ray/x-ray/",
	"title": "X-Ray Console",
	"tags": [],
	"description": "",
	"content": "We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.\nThe Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph. In the example below, we can see that the x-ray-sample-front-k8s service is processing 39 transactions per minute with an average latency of 0.99ms per operation. Additionally, the x-ray-sample-back-k8s is showing an average latency of 0.08ms per transaction.\nNext, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.\nIf you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request. In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.\nIn the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.\nClick on the image to zoom\n "
},
{
	"uri": "/x-ray/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Tracing with X-Ray module.\nThe content for this module was based on the Application Tracing on Kubernetes with AWS X-Ray blog post.\nThis module is not used in subsequent steps, so you can remove the resources now or at the end of the workshop.\nDelete the Kubernetes example microservices deployed:\nkubectl delete deployments x-ray-sample-front-k8s x-ray-sample-back-k8s kubectl delete services x-ray-sample-front-k8s x-ray-sample-back-k8s  Delete the X-Ray DaemonSet:\nkubectl delete -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  "
},
{
	"uri": "/codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": " Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in. Select Oregon (us-west-2) if you provisioned the workshow per the \u0026ldquo;Start the workshop at an AWS event\u0026rdquo; instructions.\n Once you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s  For the status of the service, run the following command:\nkubectl describe service hello-k8s  Challenge: How can we view our exposed service?\nHINT: Which kubectl command will get you the Elastic Load Balancer (ELB) endpoint for this app?\n  Expand here to see the solution   Once the service is built and delivered, we can run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\n kubectl get services hello-k8s -o wide  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n   "
},
{
	"uri": "/monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": " Download Grafana and update configuration curl -o grafana-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/grafana/values.yaml  You will make three edits to grafana-values.yaml. Search for storageClassName, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. Search for adminPassword, uncomment and change the password to \u0026ldquo;EKS!sAWSome\u0026rdquo; or something similar. Make a note of this password as you will need it for logging into grafana dashboard later\nThe third edit you will do is for adding Prometheus as a datasource. Search for datasources.yaml and uncomment entire block, update prometheus to the endpoint referred earlier by helm response. The configuration will look similar to below\ndatasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true  Now let\u0026rsquo;s expose Grafana dashboard using AWS ELB service. Search for service:, and update the value of type: ClusterIP to type: LoadBalancer\n  Expand here to see the complete yaml   rbac: create: true pspEnabled: true serviceAccount: create: true name: replicas: 1 deploymentStrategy: RollingUpdate readinessProbe: httpGet: path: /api/health port: 3000 livenessProbe: httpGet: path: /api/health port: 3000 initialDelaySeconds: 60 timeoutSeconds: 30 failureThreshold: 10 image: repository: grafana/grafana tag: 5.3.1 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName securityContext: runAsUser: 472 fsGroup: 472 downloadDashboardsImage: repository: appropriate/curl tag: latest pullPolicy: IfNotPresent ## Pod Annotations # podAnnotations: {} ## Deployment annotations # annotations: {} ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: type: LoadBalancer port: 80 annotations: {} labels: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026quot;true\u0026quot; labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ # nodeSelector: {} ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## affinity: {} ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: false storageClassName: prometheus # accessModes: # - ReadWriteOnce # size: 10Gi # annotations: {} # subPath: \u0026quot;\u0026quot; # existingClaim: adminUser: admin adminPassword: EKS!sAWSome ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Extra environment variables that will be pass onto deployment pods env: {} ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment ## This can be useful for auth tokens, etc envFromSecret: \u0026quot;\u0026quot; ## Additional grafana server secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # secretName: grafana-secret-files # readOnly: true ## Pass the plugins you want installed as a list. ## plugins: [] # - digrich-bubblechart-panel # - grafana-clock-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true ## Configure grafana dashboard providers ## ref: http://docs.grafana.org/administration/provisioning/#dashboards ## ## `path` must be /var/lib/grafana/dashboards/\u0026lt;provider_name\u0026gt; ## dashboardProviders: {} # dashboardproviders.yaml: # apiVersion: 1 # providers: # - name: 'default' # orgId: 1 # folder: '' # type: file # disableDeletion: false # editable: true # options: # path: /var/lib/grafana/dashboards/default ## Configure grafana dashboard to import ## NOTE: To use dashboards you must also enable/configure dashboardProviders ## ref: https://grafana.com/dashboards ## ## dashboards per provider, use provider name as key. ## dashboards: {} # default: # some-dashboard: # json: | # $RAW_JSON # prometheus-stats: # gnetId: 2 # revision: 2 # datasource: Prometheus # local-dashboard: # url: https://example.com/repository/test.json ## Reference to external ConfigMap per provider. Use provider name as key and ConfiMap name as value. ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both. ## ConfigMap data example: ## ## data: ## example-dashboard.json: | ## RAW_JSON ## dashboardsConfigMaps: {} # default: \u0026quot;\u0026quot; ## Grafana's primary configuration ## NOTE: values in map will be converted to ini format ## ref: http://docs.grafana.org/installation/configuration/ ## grafana.ini: paths: data: /var/lib/grafana/data logs: /var/log/grafana plugins: /var/lib/grafana/plugins provisioning: /etc/grafana/provisioning analytics: check_for_updates: true log: mode: console grafana_net: url: https://grafana.net ## LDAP Authentication can be enabled with the following values on grafana.ini ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid # auth.ldap: # enabled: true # allow_sign_up: true # config_file: /etc/grafana/ldap.toml ## Grafana's LDAP configuration ## Templated by the template in _helpers.tpl ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap ## ref: http://docs.grafana.org/installation/ldap/#configuration ldap: # `existingSecret` is a reference to an existing secret containing the ldap configuration # for Grafana in a key `ldap-toml`. existingSecret: \u0026quot;\u0026quot; # `config` is the content of `ldap.toml` that will be stored in the created secret config: \u0026quot;\u0026quot; # config: |- # verbose_logging = true # [[servers]] # host = \u0026quot;my-ldap-server\u0026quot; # port = 636 # use_ssl = true # start_tls = false # ssl_skip_verify = false # bind_dn = \u0026quot;uid=%s,ou=users,dc=myorg,dc=com\u0026quot; ## Grafana's SMTP configuration ## NOTE: To enable, grafana.ini must be configured with smtp.enabled ## ref: http://docs.grafana.org/installation/configuration/#smtp smtp: # `existingSecret` is a reference to an existing secret containing the smtp configuration # for Grafana in keys `user` and `password`. existingSecret: \u0026quot;\u0026quot; ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards sidecar: image: kiwigrid/k8s-sidecar:0.0.3 imagePullPolicy: IfNotPresent resources: # limits: # cpu: 100m # memory: 100Mi # requests: # cpu: 50m # memory: 50Mi dashboards: enabled: false # label that the configmaps with dashboards are marked with label: grafana_dashboard # folder in the pod that should hold the collected dashboards folder: /tmp/dashboards datasources: enabled: false # label that the configmaps with datasources are marked with label: grafana_datasource    Deploy grafana helm install -f grafana-values.yaml stable/grafana --name grafana --namespace grafana  Run the command to check if Grafana is running properly\nkubectl get all -n grafana  You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-b9697f8b5-t9w4j 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.49.172 abe57f85de73111e899cf0289f6dc4a4-1343235144.us-west-2.elb.amazonaws.com 80:31570/TCP 3m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1 1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-b9697f8b5 1 1 1 2m  You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') echo \u0026quot;http://$ELB\u0026quot;  "
},
{
	"uri": "/statefulset/services/",
	"title": "Create Services",
	"tags": [],
	"description": "",
	"content": " Introduction Kubernetes Service defines a logical set of Pods and a policy by which to access them. Service can be exposed in different ways by specifying a type in the serviceSpec. StatefulSet currently requires a Headless Service to control the domain of its Pods, directly reach each Pod with stable DNS entries. By specifying \u0026ldquo;None\u0026rdquo; for the clusterIP, you can create Headless Service.\nCreate Services Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/services.files/mysql-services.yml  Check the configuration of mysql-services.yml by following command.\ncat ~/environment/templates/mysql-services.yml  You can see the mysql service is for DNS resolution so that when pods are placed by StatefulSet controller, pods can be resolved using pod-name.mysql. mysql-read is a client service that does load balancing for all slaves.\n# Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql  Create service mysql and mysql-read by following command\nkubectl create -f ~/environment/templates/mysql-services.yml    Related files   mysql-services.yml  (0 ko)    "
},
{
	"uri": "/codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": " Update Our Application So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console Confirm the Change If you still have the ELB URL open in your browser, refresh to confirm the update. If you need to retrieve the URL again, use kubectl get services hello-k8s -o wide\n"
},
{
	"uri": "/codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the CI/CD with CodePipeline module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s  Next, we are going to delete the CloudFormation stack created. Open CloudFormation the AWS Management Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete stack:\nNow we are going to delete the ECR repository:\nEmpty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, select the bucket, then empty the bucket and finally delete the bucket:\nFinally, we are going to delete the IAM role created for CodeBuild to permit changes to the EKS cluster:\naws iam delete-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe aws iam delete-role --role-name EksWorkshopCodeBuildKubectlRole  "
},
{
	"uri": "/advanced-networking/secondary_cidr/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Before we configure EKS, we need to enable secondary CIDR blocks in your VPC and make sure they have proper tags and route table configurations\nAdd secondary CIDRs to your VPC There are restrictions on the range of secondary CIDRs you can use to extend your VPC. For more info, see IPv4 CIDR Block Association Restrictions\n You can use below commands to add 100.64.0.0/16 to your EKS cluster VPC. Please note to change the Values parameter to EKS cluster name if you used different name than eksctl-eksworkshop\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') aws ec2 associate-vpc-cidr-block --vpc-id $VPC_ID --cidr-block 100.64.0.0/16  Next step is to create subnets. Before we do this step, let\u0026rsquo;s check how many subnets we are consuming. You can run this command to see EC2 instance and AZ details\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  I have 3 instances and using 3 subnets in my environment. For simplicity, we will use the same AZ\u0026rsquo;s and create 3 secondary CIDR subnets but you can certainly customize according to your networking requirements. Remember to change the AZ names according to your environment\nexport AZ1=us-east-2a export AZ2=us-east-2b export AZ3=us-east-2c CGNAT_SNET1=$(aws ec2 create-subnet --cidr-block 100.64.0.0/19 --vpc-id $VPC_ID --availability-zone $AZ1 | jq -r .Subnet.SubnetId) CGNAT_SNET2=$(aws ec2 create-subnet --cidr-block 100.64.32.0/19 --vpc-id $VPC_ID --availability-zone $AZ2 | jq -r .Subnet.SubnetId) CGNAT_SNET3=$(aws ec2 create-subnet --cidr-block 100.64.64.0/19 --vpc-id $VPC_ID --availability-zone $AZ3 | jq -r .Subnet.SubnetId)  Next step is to add Kubernetes tags on newer Subnets. You can check these tags by querying your current subnets\naws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 --output text  Output shows similar to this\nTAGS aws:cloudformation:logical-id SubnetPublicUSEAST2C TAGS kubernetes.io/role/elb 1 TAGS eksctl.cluster.k8s.io/v1alpha1/cluster-name eksworkshop-eksctl TAGS Name eksctl-eksworkshop-eksctl-cluster/SubnetPublicUSEAST2C TAGS aws:cloudformation:stack-name eksctl-eksworkshop-eksctl-cluster TAGS kubernetes.io/cluster/eksworkshop-eksctl shared TAGS aws:cloudformation:stack-id arn:aws:cloudformation:us-east-2:012345678901:stack/eksctl-eksworkshop-eksctl-cluster/8da51fc0-2b5e-11e9-b535-022c6f51bf82  Here are the commands to add tags to both the subnets\naws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/role/elb,Value=1  As next step, we need to associate three new subnets into a route table. Again for simplicity, we chose to add new subnets to the Public route table that has connectivity to Internet Gateway\nSNET1=$(aws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 | jq -r .Subnets[].SubnetId) RTASSOC_ID=$(aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=$SNET1 | jq -r .RouteTables[].RouteTableId) aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET1 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET2 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET3  "
},
{
	"uri": "/batch/jobs/",
	"title": "Kubernetes Jobs",
	"tags": [],
	"description": "",
	"content": " Kubernetes Jobs A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.\nSave the below manifest as \u0026lsquo;job-whalesay.yaml\u0026rsquo; using your favorite editor.\napiVersion: batch/v1 kind: Job metadata: name: whalesay spec: template: spec: containers: - name: whalesay image: docker/whalesay command: [\u0026quot;cowsay\u0026quot;, \u0026quot;This is a Kubernetes Job!\u0026quot;] restartPolicy: Never backoffLimit: 4  Run a sample Kubernetes Job using the whalesay image.\nkubectl apply -f job-whalesay.yaml  Wait until the job has completed successfully.\nkubectl get job/whalesay  NAME DESIRED SUCCESSFUL AGE whalesay 1 1 2m  Confirm the output.\nkubectl logs -l job-name=whalesay  ___________________________ \u0026lt; This is a Kubernetes Job! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_appmesh/create_mesh/",
	"title": "Create the App Mesh",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll first create the actual App Mesh. Copy and paste the following into your terminal to create an App Mesh called APP_MESH_DEMO.\naws appmesh create-mesh --mesh-name APP_MESH_DEMO  "
},
{
	"uri": "/servicemesh_with_istio/download/",
	"title": "Download and Install Istio CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring Istio we’ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl -L https://git.io/getLatestIstio | sh - // version can be different as istio gets upgraded cd istio-* sudo mv -v bin/istioctl /usr/local/bin/  "
},
{
	"uri": "/monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": " Create Dashboards Login into Grafana dashboard using credentials supplied during configuration\nYou will notice that \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;create your first data source\u0026rsquo; are already completed. We will import community created dashboard for this tutorial\nClick \u0026lsquo;+\u0026rsquo; button on left panel and select \u0026lsquo;Import\u0026rsquo;\nEnter 3131 dashboard id under Grafana.com Dashboard \u0026amp; click \u0026lsquo;Load\u0026rsquo;.\nLeave the defaults, select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down, click \u0026lsquo;Import\u0026rsquo;.\nThis will show monitoring dashboard for all cluster nodes\nFor creating dashboard to monitor all pods, repeat same process as above and enter 3146 for dashboard id\n"
},
{
	"uri": "/introduction/basics/",
	"title": "Kubernetes (k8s) Basics",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n What is Kubernetes?   Kubernetes Nodes   K8s Objects Overview   K8s Objects Detail (1/2)   K8s Objects Detail (2/2)   "
},
{
	"uri": "/deploy/deploycrystal/",
	"title": "Deploy Crystal Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Crystal Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-crystal  "
},
{
	"uri": "/cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments and kubernetes dashboard.\nNote that if you followed the cleanup section of the modules, some of the commands below might fail because there is nothing to delete and its ok.\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  "
},
{
	"uri": "/eksctl/launcheks/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": " DO NOT PROCEED with this step unless you have validated the IAM role in use by the Cloud9 IDE. You will not be able to run the necessary kubectl commands in the later modules unless the EKS cluster is built using the IAM role.\n Challenge: How do I check the IAM role on the workspace?\n  Expand here to see the solution   Run aws sts get-caller-identity and validate that your Arn contains eksworkshop-admin or modernizer-workshop-cl9  (or the role created when starting the workshop) and an Instance Id.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin/i-01234567890abcdef\u0026quot; } or { \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/modernizer-workshop-cl9/i-01234567890abcdef\u0026quot; }  If you do not see the correct role, please go back and validate the IAM role for troubleshooting.\nIf you do see the correct role, proceed to next step to create an EKS cluster.\n  Create an EKS cluster To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/spotworkers/deployhandler/",
	"title": "Deploy The Spot Interrupt Handler",
	"tags": [],
	"description": "",
	"content": " In this section, we will prepare our cluster to handle Spot interruptions.\nIf the available On-Demand capacity of a particular instance type is depleted, the Spot Instance is sent an interruption notice two minutes ahead to gracefully wrap up things. We will deploy a pod on each spot instance to detect and redeploy applications elsewhere in the cluster\nThe first thing that we need to do is deploy the Spot Interrupt Handler on each Spot Instance. This will monitor the EC2 metadata service on the instance for a interruption notice.\nThe workflow can be summarized as:\n Identify that a Spot Instance is being reclaimed. Use the 2-minute notification window to gracefully prepare the node for termination. Taint the node and cordon it off to prevent new pods from being placed. Drain connections on the running pods. Replace the pods on remaining nodes to maintain the desired capacity.  We have provided an example K8s DaemonSet manifest. A DaemonSet runs one pod per node.\nmkdir ~/environment/spot cd ~/environment/spot wget https://eksworkshop.com/spot/managespot/deployhandler.files/spot-interrupt-handler-example.yml  As written, the manifest will deploy pods to all nodes including On-Demand, which is a waste of resources. We want to edit our DaemonSet to only be deployed on Spot Instances. Let\u0026rsquo;s use the labels to identify the right nodes.\nUse a nodeSelector to constrain our deployment to spot instances. View this link for more details.\nChallenge Configure our Spot Handler to use nodeSelector   Expand here to see the solution   Place this at the end of the DaemonSet manifest under Spec.Template.Spec.nodeSelector\nnodeSelector: lifecycle: Ec2Spot   \nDeploy the DaemonSet\nkubectl apply -f ~/environment/spot/spot-interrupt-handler-example.yml  If you receive an error deploying the DaemonSet, there is likely a small error in the YAML file. We have provided a solution file at the bottom of this page that you can use to compare.\n View the pods. There should be one for each spot node.\nkubectl get daemonsets    Related files   spot-interrupt-handler-example.yml  (1 ko)   spot-interrupt-handler-solution.yml  (1 ko)    "
},
{
	"uri": "/statefulset/statefulset/",
	"title": "Create StatefulSet",
	"tags": [],
	"description": "",
	"content": " Introduction StatefulSet consists of serviceName, replicas, template and volumeClaimTemplates. serviceName is \u0026ldquo;mysql\u0026rdquo;, headless service we created in previous section, replicas is 3, the desired number of pod, template is the configuration of pod, volumeClaimTemplates is to claim volume for pod based on storageClassName, gp2 that we created in \u0026ldquo;Define Storageclass\u0026rdquo; section. Percona Xtrabackup is in template to clone source MySQL server to its slaves.\nCreate StatefulSet Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml  Create statefulset \u0026ldquo;mysql\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-statefulset.yml  Watch StatefulSet Watch the status of statefulset.\nkubectl get -w statefulset  It will take few minutes for pods to initialize and have statefulset created. DESIRED is the replicas number you define at StatefulSet.\nNAME DESIRED CURRENT AGE mysql 3 1 8s mysql 3 2 59s mysql 3 3 2m mysql 3 3 3m  Open another Cloud9 Terminal and watch the progress of pods creation using the following command.\nkubectl get pods -l app=mysql --watch  You can see ordered, graceful deployment with a stable, unique name for each pod.\nNAME READY STATUS RESTARTS AGE mysql-0 0/2 Init:0/2 0 30s mysql-0 0/2 Init:1/2 0 35s mysql-0 0/2 PodInitializing 0 47s mysql-0 1/2 Running 0 48s mysql-0 2/2 Running 0 59s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 35s mysql-1 0/2 Init:1/2 0 45s mysql-1 0/2 PodInitializing 0 54s mysql-1 1/2 Running 0 55s mysql-1 2/2 Running 0 1m mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 32s mysql-2 0/2 Init:1/2 0 43s mysql-2 0/2 PodInitializing 0 50s mysql-2 1/2 Running 0 52s mysql-2 2/2 Running 0 56s  Press Ctrl+C to stop watching.\nCheck the dynamically created PVC by following command.\nkubectl get pvc -l app=mysql  You can see data-mysql-0,1,2 are created by STORAGECLASS mysql-gp2.\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d  (Optional) Check 10Gi 3 EBS volumes are created across availability zones at your AWS console.   Related files   mysql-statefulset.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/customize/",
	"title": "Customize Defaults",
	"tags": [],
	"description": "",
	"content": " If you look in the newly created eksdemo directory, you\u0026rsquo;ll see several files and directories. Specifically, inside the /templates directory, you\u0026rsquo;ll see:\n NOTES.txt: The “help text” for your chart. This will be displayed to your users when they run helm install. deployment.yaml: A basic manifest for creating a Kubernetes deployment service.yaml: A basic manifest for creating a service endpoint for your deployment _helpers.tpl: A place to put template helpers that you can re-use throughout the chart  We\u0026rsquo;re actually going to create our own files, so we\u0026rsquo;ll delete these boilerplate files\nrm -rf ~/environment/eksdemo/templates/ rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml  Create a new Chart.yaml file which will describe the chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v1 appVersion: \u0026quot;1.0\u0026quot; description: A Helm chart for EKS Workshop Microservices application name: eksdemo version: 0.1.0 EoF  Next we\u0026rsquo;ll copy the manifest files for each of our microservices into the templates directory as servicename.yaml\n#create subfolders for each template type mkdir -p ~/environment/eksdemo/templates/deployment mkdir -p ~/environment/eksdemo/templates/service # Copy and rename frontend manifests cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copy and rename crystal manifests cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copy and rename nodejs manifests cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml  All files in the templates directory are sent through the template engine. These are currently plain YAML files that would be sent to Kubernetes as-is.\nReplace hard-coded values with template directives Let\u0026rsquo;s replace some of the values with template directives to enable more customization by removing hard-coded values.\nOpen ~/environment/eksdemo/templates/deployment/frontend.yaml in your Cloud9 editor.\nThe following steps should be completed seperately for frontend.yaml, crystal.yaml, and nodejs.yaml.\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replicas }}  Under spec.template.spec.containers.image, replace the image with the correct template value from the table below:\n   Filename Value     frontend.yaml - image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml - image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml - image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Create a values.yaml file with our template defaults This file will populate our template directives with default values.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Default values for eksdemo. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Release-wide Values replicas: 3 version: 'latest' # Service Specific Values nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF  "
},
{
	"uri": "/logging/setup_es/",
	"title": "Provision an Elasticsearch Cluster",
	"tags": [],
	"description": "",
	"content": "This example creates a two instance Amazon Elasticsearch cluster named kubernetes-logs. This cluster is created in the same region as the Kubernetes cluster and CloudWatch log group.\nNote that this cluster has an open access policy which will need to be locked down in production environments.\n aws es create-elasticsearch-domain \\ --domain-name kubernetes-logs \\ --elasticsearch-version 6.3 \\ --elasticsearch-cluster-config \\ InstanceType=m4.large.elasticsearch,InstanceCount=2 \\ --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \\ --access-policies '{\u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;,\u0026quot;Statement\u0026quot;:[{\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;*\u0026quot;]},\u0026quot;Action\u0026quot;:[\u0026quot;es:*\u0026quot;],\u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;}]}'  It takes a little while for the cluster to be created and arrive at an active state. The AWS Console should show the following status when the cluster is ready.\nYou could also check this via AWS CLI:\naws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'  If the output value is false that means the domain has been processed and is now available to use.\nFeel free to move on to the next section for now.\n"
},
{
	"uri": "/scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an application and expose as a service on TCP port 80. The application is a custom-built image based on the php-apache image. The index.php page performs calculations to generate CPU load. More information can be found here\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80  Create an HPA resource This HPA scales up when CPU exceeds 50% of the allocated container resource.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  View the HPA using kubectl. You probably will see \u0026lt;unknown\u0026gt;/50% for 1-2 minutes and then you should be able to see 0%/50%\nkubectl get hpa  Generate load to trigger scaling Open a new terminal in the Cloud9 Environment and run the following command to drop into a shell on a new container\nkubectl run -i --tty load-generator --image=busybox /bin/sh  Execute a while loop to continue getting http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done  In the previous tab, watch the HPA with the following command\nkubectl get hpa -w  You will see HPA scale the pods from 1 up to our configured maximum (10) until the CPU average is below our target (50%)\nYou can now stop (Ctrl + C) load test that was running in the other terminal. You will notice that HPA will slowly bring the replica count to min number based on its configuration. You should also get out of load testing application by pressing Ctrl + D\n"
},
{
	"uri": "/eksctl/",
	"title": "Launch using eksctl",
	"tags": [],
	"description": "",
	"content": " Launch using eksctl by Weaveworks We have some very powerful partner tools that allow us to automate much of the experience of creating an EKS cluster, simplifying the process.\nIn this module, we will highlight a tool contributed by Weaveworks called eksctl, based on the official AWS CloudFormation templates, and will use it to launch and configure our EKS cluster and nodes.\n  "
},
{
	"uri": "/prerequisites/sshkey/",
	"title": "Create an SSH key",
	"tags": [],
	"description": "",
	"content": "Starting from here, when you see command to be entered such as below, you will enter these commands into Cloud9 IDE. You can use the Copy to clipboard feature (right hand upper corner) to simply copy and paste into Cloud9. In order to paste, you can use Ctrl + V for Windows or Command + V for Mac.\n Please run this command to generate SSH Key in Cloud9. This key will be used on the worker node instances to allow ssh access if necessary.\nssh-keygen  Press enter 3 times to take the default choices\n Upload the public key to your EC2 region:\naws ec2 import-key-pair --key-name \u0026quot;eksworkshop\u0026quot; --public-key-material file://~/.ssh/id_rsa.pub  "
},
{
	"uri": "/prerequisites/k8stools/",
	"title": "Install Kubernetes Tools",
	"tags": [],
	"description": "",
	"content": " Amazon EKS clusters require kubectl and kubelet binaries and the aws-iam-authenticator binary to allow IAM authentication for your Kubernetes cluster.\nIn this workshop we will give you the commands to download the Linux binaries. If you are running Mac OSX / Windows, please see the official EKS docs for the download links.\n Create the default ~/.kube directory for storing kubectl configuration mkdir -p ~/.kube  Install kubectl sudo curl --silent --location -o /usr/local/bin/kubectl \u0026quot;https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x /usr/local/bin/kubectl  Install AWS IAM Authenticator go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator sudo mv ~/go/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator  Verify the binaries kubectl version --short --client aws-iam-authenticator help  Install JQ sudo yum -y install jq  "
},
{
	"uri": "/prerequisites/clone/",
	"title": "Clone the Service Repos",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/brentley/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git  "
},
{
	"uri": "/monitoring/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Delete Prometheus and grafana helm delete prometheus helm del --purge prometheus helm delete grafana helm del --purge grafana  "
},
{
	"uri": "/deploy/servicetype/",
	"title": "Let&#39;s check Service Types",
	"tags": [],
	"description": "",
	"content": "Before we bring up the frontend service, let\u0026rsquo;s take a look at the service types we are using: This is kubernetes/service.yaml for our frontend service:\napiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  Notice type: LoadBalancer: This will configure an ELB to handle incoming traffic to this service.\nCompare this to kubernetes/service.yaml for one of our backend services:\napiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  Notice there is no specific service type described. When we check the kubernetes documentation we find that the default type is ClusterIP. This Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster.\n"
},
{
	"uri": "/statefulset/testmysql/",
	"title": "Test MySQL",
	"tags": [],
	"description": "",
	"content": "You can use mysql-client to send some data to the master, mysql-0.mysql by following command.\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\ mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello, from mysql-client'); EOF  Run the following to test slaves (mysql-read) received the data.\nkubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never --\\ mysql -h mysql-read -e \u0026quot;SELECT * FROM test.messages\u0026quot;  The output should look like this.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  To test load balancing across slaves, run the following command.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  Each MySQL instance is assigned a unique identifier, and it can be retrieved using @@server_id. It will print the server id serving the request and the timestamp.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 12:44:57 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:44:58 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:44:59 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:45:00 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:45:01 | +-------------+---------------------+  Leave this open in a separate window while you test failure in the next section.\n"
},
{
	"uri": "/healthchecks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Our Liveness Probe example used HTTP request and Readiness Probe executed a command to check health of a pod. Same can be accomplished using a TCP request as described in the documentation.\n kubectl delete -f ~/environment/healthchecks/liveness-app.yaml kubectl delete -f ~/environment/healthchecks/readiness-deployment.yaml  "
},
{
	"uri": "/prerequisites/self_paced/iamrole/",
	"title": "Create an IAM role for your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next to view permissions. Confirm that AdministratorAccess is checked, then click Next to review. Enter eksworkshop-admin for the Name, and select Create Role   "
},
{
	"uri": "/prerequisites/self_paced/ec2instance/",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to find your Cloud9 EC2 instance Select the instance, then choose Actions / Instance Settings / Attach/Replace IAM Role  Choose eksworkshop-admin from the IAM Role drop down, and select Apply   "
},
{
	"uri": "/deploy/servicerole/",
	"title": "Ensure the ELB Service Role exists",
	"tags": [],
	"description": "",
	"content": "In AWS accounts that have never created a load balancer before, it\u0026rsquo;s possible that the service role for ELB might not exist yet.\nWe can check for the role, and create it if it\u0026rsquo;s missing.\nCopy/Paste the following commands into your Cloud9 workspace:\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot;  "
},
{
	"uri": "/advanced-networking/secondary_cidr/configure-cni/",
	"title": "Configure CNI",
	"tags": [],
	"description": "",
	"content": " Before we start making changes to VPC CNI, let\u0026rsquo;s make sure we are using latest CNI version\nRun this command to find CNI version\nkubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026quot;/\u0026quot; -f 2  Here is a sample response\namazon-k8s-cni:1.2.1  Upgrade version to 1.3 if you have an older version\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Wait till all the pods are recycled. You can check the status of pods by using this command\nkubectl get pods -n kube-system -w  Configure Custom networking Edit aws-node configmap and add AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG environment variable to the node container spec and set it to true\nNote: You only need to add two lines into configmap\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Save the file and exit your text editor\nTerminate worker nodes so that Autoscaling launches newer nodes that come bootstrapped with custom network config\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  "
},
{
	"uri": "/batch/install/",
	"title": "Install Argo CLI",
	"tags": [],
	"description": "",
	"content": " Install Argo CLI Before we can get started configuring argo we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\nsudo curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo/releases/download/v2.2.1/argo-linux-amd64 sudo chmod +x /usr/local/bin/argo  "
},
{
	"uri": "/servicemesh_with_appmesh/create_virtual_nodes/",
	"title": "Create Virtual Nodes",
	"tags": [],
	"description": "",
	"content": " Next, we\u0026rsquo;ll create five Virtual Nodes, one for each of the microservices in our application.\nMore about Virtual Nodes A virtual node acts as a logical pointer to a k8s service.\nService Discovery / DNS name of the virtual node is defined in the serviceDiscovery.dns attribute.\nInbound traffic parameters for the virtual node are specified in the listener attribute.\nOutbound traffic the virtual node forwards to should be specified in the backend attribute.\ncolorgateway-vn We\u0026rsquo;ll first define the virtual node colorgateway-vn. colorgateway-vn will be the entrypoint to our app.\nThe following JSON defines:\n a virtual node named colorgateway-vn accessible via the hostname colorgateway.default.svc.cluster.local listening on port 9080 forwards to a k8s service (backend) named colorteller.default.svc.cluster.local  Copy and paste the following into your terminal to create the colorgateway virtual node:\naws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorgateway.default.svc.cluster.local\u0026quot; } }, \u0026quot;backends\u0026quot;: [ \u0026quot;colorteller.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorgateway-vn\u0026quot; }'  colorteller-vn collorteller-vn always white as its color response.\nThe following JSON defines:\n a virtual node named colorteller-vn accessible via the hostname colorteller.default.svc.cluster.local listening on port 9080  Copy and paste the following into your terminal to create the colorteller-vn virtual node:\naws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-vn\u0026quot; }'  Similarly, colorteller-black-vn, colorteller-blue-vn, and colorteller-red-vn return the colors black, blue, and red respectively.\nCopy and paste these three virtual node definitions into your terminal to create the black, blue, and red collorteller virtual nodes:\ncolorteller-black-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-black.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot; }'  colorteller-blue-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-blue.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-blue-vn\u0026quot; }'  colorteller-red-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-red.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot; }'  "
},
{
	"uri": "/servicemesh_with_istio/install/",
	"title": "Install Istio",
	"tags": [],
	"description": "",
	"content": " Install Istio\u0026rsquo;s CRD The Custom Resource Definition, also known as a CRD, is an API resource which allows you to define custom resources.\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml  Install Istio Helm is required for the following examples. If you have not installed Helm yet, please first reference the Helm chapter before proceeding.\nkubectl create -f install/kubernetes/helm/helm-service-account.yaml helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set global.configValidation=false --set sidecarInjectorWebhook.enabled=false --set grafana.enabled=true --set servicegraph.enabled=true \u0026gt; istio.yaml kubectl create namespace istio-system kubectl apply -f istio.yaml  Watch the progress of installation using:\nkubectl get pod -n istio-system -w  And hit CTRL-C when you\u0026rsquo;re ready to proceed.\nNAME READY STATUS RESTARTS AGE grafana-9cfc9d4c9-csvw7 1/1 Running 0 3m istio-citadel-6d7f9c545b-w7hjs 1/1 Running 0 3m istio-cleanup-secrets-vrkm5 0/1 Completed 0 3m istio-egressgateway-866885bb49-cz6jr 1/1 Running 0 3m istio-galley-6d74549bb9-t8sqb 1/1 Running 0 3m istio-grafana-post-install-4bgxv 0/1 Completed 0 3m istio-ingressgateway-6c6ffb7dc8-dnmqx 1/1 Running 0 3m istio-pilot-685fc95d96-jhfhv 2/2 Running 0 3m istio-policy-688f99c9c4-pb558 2/2 Running 0 3m istio-security-post-install-5dw8n 0/1 Completed 0 3m istio-telemetry-69b794ff59-spkp2 2/2 Running 0 3m prometheus-f556886b8-cxb9n 1/1 Running 0 3m servicegraph-778f94d6f8-tfmp6 1/1 Running 0 3m  "
},
{
	"uri": "/introduction/basics/what_is_k8s/",
	"title": "What is Kubernetes?",
	"tags": [],
	"description": "",
	"content": " Builds on over a decade of experience and best practices Utilizes declarative configuration and automation Draws upon a large ecosystem of tools, services, support  More information on what Kubernetes is all about can be found on the official Kubernetes website.\n"
},
{
	"uri": "/deploy/deployfrontend/",
	"title": "Deploy Frontend Service",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Ruby Frontend!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-frontend  "
},
{
	"uri": "/spotworkers/preferspot/",
	"title": "Deploy an Application on Spot",
	"tags": [],
	"description": "",
	"content": " We are redesigning our Microservice example and want our frontend service to be deployed on Spot Instances when they are available. We will use Node Affinity in our manifest file to configure this.\nConfigure Node Affinity and Tolerations Open the deployment manifest in your Cloud9 editor - ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml\nEdit the spec to configure NodeAffinity to prefer Spot Instances, but not require them. This will allow the pods to be scheduled on On-Demand nodes if no spot instances were available or correctly labelled.\nWe also want to configure a toleration which will allow the pods to \u0026ldquo;tolerate\u0026rdquo; the taint that we configured on our EC2 Spot Instances.\nFor examples of Node Affinity, check this link\nFor examples of Taints and Tolerations, check this link\nChallenge Configure Affinity and Toleration\n  Expand here to see the solution   Add this to your deployment file under spec.template.spec\naffinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: lifecycle operator: In values: - Ec2Spot tolerations: - key: \u0026quot;spotInstance\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;true\u0026quot; effect: \u0026quot;PreferNoSchedule\u0026quot;  We have provided a solution file below that you can use to compare.\n     Related files   deployment-solution.yml  (1 ko)    Redeploy the Frontend on Spot First let\u0026rsquo;s take a look at all pods deployed on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  Now we will redeploy our microservices with our edited Frontend Manifest\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml  We can again check all pods deployed on Spot Instances and should now see the frontend pods running on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  "
},
{
	"uri": "/cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "In order to delete the resources created for this EKS cluster, run the following commands:\nDelete the cluster:\neksctl delete cluster --name=eksworkshop-eksctl  The nodegroup will have to complete the deletion process before the EKS cluster can be deleted. The total process will take approximately 15 minutes, and can be monitored via the CloudFormation Console\n "
},
{
	"uri": "/eksctl/test/",
	"title": "Test the Cluster",
	"tags": [],
	"description": "",
	"content": " Confirm your Nodes:\nkubectl get nodes  Export the Worker Role Name for use throughout the workshop\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile  Congratulations! You now have a fully working Amazon EKS Cluster that is ready to use!\n"
},
{
	"uri": "/dashboard/connect/",
	"title": "Access the Dashboard",
	"tags": [],
	"description": "",
	"content": "Now we can access the Kubernetes Dashboard\n In your Cloud9 environment, click Tools / Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  Open a New Terminal Tab and enter\naws-iam-authenticator token -i eksworkshop-eksctl --token-only  Copy the output of this command and then click the radio button next to Token then in the text field below paste the output from the last command.\nThen press Sign In.\nIf you want to see the dashboard in a full tab, click the Pop Out button, like below: "
},
{
	"uri": "/scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": " Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:\n One Auto Scaling group - This is what we will use Multiple Auto Scaling groups Auto-Discovery Master Node setup  Configure the Cluster Autoscaler (CA) We have provided a manifest file to deploy the CA. Copy the commands below into your Cloud9 Terminal.\nmkdir ~/environment/cluster-autoscaler cd ~/environment/cluster-autoscaler wget https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml  Configure the ASG We will need to provide the name of the Autoscaling Group that we want CA to manipulate. Collect the name of the Auto Scaling Group (ASG) containing your worker nodes. Record the name somewhere. We will use this later in the manifest file.\nYou can find it in the console by following this link.\nCheck the box beside the ASG and click Actions and Edit\nChange the following settings:\n Min: 2 Max: 8  Click Save\nConfigure the Cluster Autoscaler Using the file browser on the left, open cluster_autoscaler.yml\nSearch for command: and within this block, replace the placeholder text \u0026lt;AUTOSCALING GROUP NAME\u0026gt; with the ASG name that you copied in the previous step. Also, update AWS_REGION value to reflect the region you are using and Save the file.\ncommand: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --nodes=2:8:eksctl-eksworkshop-eksctl-nodegroup-0-NodeGroup-SQG8QDVSR73G env: - name: AWS_REGION value: us-east-1  This command contains all of the configuration for the Cluster Autoscaler. The primary config is the --nodes flag. This specifies the minimum nodes (2), max nodes (8) and ASG Name.\nAlthough Cluster Autoscaler is the de facto standard for automatic scaling in K8s, it is not part of the main release. We deploy it like any other pod in the kube-system namespace, similar to other management pods.\nCreate an IAM Policy We need to configure an inline policy and add it to the EC2 instance profile of the worker nodes\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  mkdir ~/environment/asg_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/asg_policy/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker --policy-document file://~/environment/asg_policy/k8s-asg-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker  Deploy the Cluster Autoscaler kubectl apply -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml  Watch the logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  We are now ready to scale our cluster   Related files   cluster_autoscaler.yml  (3 ko)    "
},
{
	"uri": "/logging/deploy/",
	"title": "Deploy Fluentd",
	"tags": [],
	"description": "",
	"content": "mkdir ~/environment/fluentd cd ~/environment/fluentd wget https://eksworkshop.com/logging/deploy.files/fluentd.yml  Explore the fluentd.yml to see what is being deployed. There is a link at the bottom of this page. The Fluentd log agent configuration is located in the Kubernetes ConfigMap. Fluentd will be deployed as a DaemonSet, i.e. one pod per worker node. In our case, a 3 node cluster is used and so 3 pods will be shown in the output when we deploy.\nUpdate REGION and CLUSTER_NAME environment variables in fluentd.yml as required. They are set to us-west-2 and eksworkshop-eksctl by default.\n kubectl apply -f ~/environment/fluentd/fluentd.yml  Watch for all of the pods to change to running status\nkubectl get pods -w --namespace=kube-system  We are now ready to check that logs are arriving in CloudWatch Logs\nSelect the region that is mentioned in fluentd.yml to browse the Cloudwatch Log Group if required.\n  Related files   fluentd.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/deploy/",
	"title": "Deploy the eksdemo Chart",
	"tags": [],
	"description": "",
	"content": " Use the dry-run flag to test our templates To test the syntax and validity of the Chart without actually deploying it, we\u0026rsquo;ll use the dry-run flag.\nThe following command will build and output the rendered templates without installing the Chart:\nhelm install --debug --dry-run --name workshop ~/environment/eksdemo  Confirm that the values created by the template look correct.\nDeploy the chart Now that we have tested our template, lets install it.\nhelm install --name workshop ~/environment/eksdemo  Similar to what we saw previously in the NGINX Helm Chart example, an output of the Deployment, Pod, and Service objects are output, similar to:\nNAME: workshop LAST DEPLOYED: Fri Nov 16 21:42:00 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME AGE ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Deployment ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE ecsdemo-crystal-764b9cb9bc-4dwqt 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-hcb62 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-vl7nr 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-2xrtb 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-bfnc5 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-rb6rg 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-994cq 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-9qtbm 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-s9zkh 0/1 ContainerCreating 0 0s  "
},
{
	"uri": "/statefulset/testfailure/",
	"title": "Test Failure",
	"tags": [],
	"description": "",
	"content": " Unhealthy container MySQL container uses readiness probe by running mysql -h 127.0.0.1 -e \u0026lsquo;SELECT 1\u0026rsquo; on the server to make sure MySQL server is still active. Open a new terminal and simulate MySQL as being unresponsive by following command.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off  This command renames the /usr/bin/mysql command so that readiness probe can\u0026rsquo;t find it. During the next health check, the pod should report one of it\u0026rsquo;s containers is not healthy. This can be verified by following command.\nkubectl get pod mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 1/2 Running 0 12m  mysql-read load balancer detects failures and takes action by not sending traffic to the failed container, @@server_id 102. You can check this by the loop running in separate window from previous section. The loop shows the following output.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:49 | +-------------+---------------------+  Revert back to its initial state at the previous terminal.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql  Check the status again to see that both containers are running and healthy\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Running 0 5h  The loop in another terminal is now showing @@server_id 102 is back and all three servers are running. Press Ctrl+C to stop watching.\nFailed pod To simulate a failed pod, delete mysql-2 pod by following command.\nkubectl delete pod mysql-2  pod \u0026quot;mysql-2\u0026quot; deleted  StatefulSet controller recognizes failed pod and creates a new one to maintain the number of replicas with them same name and link to the same PersistentVolumeClaim.\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Pending 0 0s mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 10s mysql-2 0/2 PodInitializing 0 11s mysql-2 1/2 Running 0 12s mysql-2 2/2 Running 0 16s  Press Ctrl+C to stop watching.\n"
},
{
	"uri": "/dashboard/",
	"title": "Deploy the Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": " Deploy the Kubernetes Dashboard In this Chapter, we will deploy the official Kubernetes dashboard, and connect through our Cloud9 Workspace.\n"
},
{
	"uri": "/prerequisites/workspaceiam/",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the aws-iam-authenticator plugin, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the sprocket, or launch a new tab to open the Preferences tab Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab  To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials  We should configure our aws cli with our current region as default:\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region') echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region   Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\nFirst, get the IAM role name from the AWS CLI.\nINSTANCE_PROFILE_NAME=`basename $(aws ec2 describe-instances --filters Name=tag:Name,Values=aws-cloud9-${C9_PROJECT}-${C9_PID} | jq -r '.Reservations[0].Instances[0].IamInstanceProfile.Arn' | awk -F \u0026quot;/\u0026quot; \u0026quot;{print $2}\u0026quot;)` aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME --query \u0026quot;InstanceProfile.Roles[0].RoleName\u0026quot; --output text  The output is the role name.\neksworkshop-admin or modernizer-workshop-cl9  Compare that with the result of\naws sts get-caller-identity  VALID If the Arn contains the role name from above and an Instance ID, you may proceed.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin/i-01234567890abcdef\u0026quot; } or { \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/modernizer-workshop-cl9/i-01234567890abcdef\u0026quot; }  INVALID If the _Arn contains TeamRole, MasterRole, or does not match the role name, DO NOT PROCEED. Go back and confirm the steps on this page.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/TeamRole/MasterRole\u0026quot; }  "
},
{
	"uri": "/deploy/",
	"title": "Deploy the Example Microservices",
	"tags": [],
	"description": "",
	"content": " Deploy the Example Microservices  Deploy our Sample Applications   Deploy NodeJS Backend API   Deploy Crystal Backend API   Let\u0026#39;s check Service Types   Ensure the ELB Service Role exists   Deploy Frontend Service   Find the Service Address   Scale the Backend Services   Scale the Frontend   Cleanup the applications   "
},
{
	"uri": "/spotworkers/",
	"title": "Using Spot Instances with EKS",
	"tags": [],
	"description": "",
	"content": " Using Spot Instances with EKS In this module, you will learn how to provision, manage, and maintain your Kubernetes clusters with Amazon EKS at any scale on Spot Instances to optimize cost and scale.\n"
},
{
	"uri": "/helm_root/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": " Kubernetes Helm Helm is a package manager for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nIn this chapter, we\u0026rsquo;ll cover installing Helm. Once installed, we\u0026rsquo;ll demonstrate how Helm can be used to deploy a simple NGINX webserver, and a more sophisticated microservice.\n"
},
{
	"uri": "/statefulset/testscaling/",
	"title": "Test Scaling",
	"tags": [],
	"description": "",
	"content": " More slaves can be added to the MySQL Cluster to increase read capacity. This can be done by following command.\nkubectl scale statefulset mysql --replicas=5  You can see the message that statefulset \u0026ldquo;mysql\u0026rdquo; scaled.\nstatefulset \u0026quot;mysql\u0026quot; scaled  Watch the progress of ordered and graceful scaling.\nkubectl get pods -l app=mysql -w  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 24m mysql-3 0/2 Init:0/2 0 8s mysql-3 0/2 Init:1/2 0 9s mysql-3 0/2 PodInitializing 0 11s mysql-3 1/2 Running 0 12s mysql-3 2/2 Running 0 16s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Init:0/2 0 0s mysql-4 0/2 Init:1/2 0 10s mysql-4 0/2 PodInitializing 0 11s mysql-4 1/2 Running 0 12s mysql-4 2/2 Running 0 17s  It may take few minutes to launch all the pods.\n Press Ctrl+C to stop watching. Open another terminal to check loop if you closed it.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  You will see 5 servers are running.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:42 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 13:56:43 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:44 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 103 | 2018-11-14 13:56:49 | +-------------+---------------------+  Verify if the newly deployed slave (mysql-3) have the same data set by following command.\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\ mysql -h mysql-3.mysql -e \u0026quot;SELECT * FROM test.messages\u0026quot;  It will show the same data that master has.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  Scale down replicas to 3 by following command.\nkubectl scale statefulset mysql --replicas=3  You can see statefulset \u0026ldquo;mysql\u0026rdquo; scaled\nstatefulset \u0026quot;mysql\u0026quot; scaled  Note that scale in doesn\u0026rsquo;t delete the data or PVCs attached to the pods. You have to delete them manually. Check scale in is completed by following command.\nkubectl get pods -l app=mysql  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 35m  Check 2 PVCs(data-mysql-3, data-mysql-4) still exist by following command.\nkubectl get pvc -l app=mysql  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-3 Bound pvc-de14acd8-e811-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 34m data-mysql-4 Bound pvc-e916c3ec-e812-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 26m  Challenge: By default, deleting a PersistentVolumeClaim will delete its associated persistent volume. What if you wanted to keep the volume? Change the reclaim policy of the PersistentVolume associated with PVC \u0026ldquo;data-mysql-3\u0026rdquo; to \u0026ldquo;Retain\u0026rdquo;. Please see Kubernetes documentation for help\n  Expand here to see the solution   Change the reclaim policy:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Retain\u0026quot;}}'  Now, if you delete the PersistentVolumeClaim data-mysql-3, you can still see the EBS volume in your AWS EC2 console, with its state as \u0026ldquo;available\u0026rdquo;.\nLet\u0026rsquo;s change the reclaim policy back to \u0026ldquo;Delete\u0026rdquo; to avoid orphaned volumes:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Delete\u0026quot;}}'    Delete data-mysql-3, data-mysql-4 by following command.\nkubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4  persistentvolumeclaim \u0026quot;data-mysql-3\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-4\u0026quot; deleted  "
},
{
	"uri": "/advanced-networking/secondary_cidr/eniconfig_crd/",
	"title": "Create CRDs",
	"tags": [],
	"description": "",
	"content": " Create custom resources for ENIConfig CRD As next step, we will add custom resources to ENIConfig custom resource definition (CRD). CRD\u0026rsquo;s are extensions of Kubernetes API that stores collection of API objects of certain kind. In this case, we will store VPC Subnet and SecurityGroup configuration information in these CRD\u0026rsquo;s so that Worker nodes can access them to configure VPC CNI plugin.\nYou should have ENIConfig CRD already installed with latest CNI version (1.3+). You can check if its installed by running this command.\nkubectl get crd  You should see response similar to this\nNAME CREATED AT eniconfigs.crd.k8s.amazonaws.com 2019-03-07T20:06:48Z  If you don\u0026rsquo;t have ENIConfig installed, you can install it by using this command\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Create custom resources for each subnet by replacing Subnet and SecurityGroup IDs. Since we created three secondary subnets, we need create three custom resources.\nHere is the template for custom resource. Notice the values for Subnet ID and SecurityGroup ID needs to be replaced with appropriate values\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: $SUBNETID1 securityGroups: - $SECURITYGROUPID1 - $SECURITYGROUPID2  Check the AZ\u0026rsquo;s and Subnet IDs for these subnets. Make note of AZ info as you will need this when you apply annotation to Worker nodes using custom network config\naws ec2 describe-subnets --filters \u0026quot;Name=cidr-block,Values=100.64.*\u0026quot; --query 'Subnets[*].[CidrBlock,SubnetId,AvailabilityZone]' --output table  -------------------------------------------------------------- | DescribeSubnets | +-----------------+----------------------------+-------------+ | 100.64.32.0/19 | subnet-07dab05836e4abe91 | us-east-2a | | 100.64.64.0/19 | subnet-0692cd08cc4df9b6a | us-east-2c | | 100.64.0.0/19 | subnet-04f960ffc8be6865c | us-east-2b | +-----------------+----------------------------+-------------+  Check your Worker Node SecurityGroup\nINSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text`) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;SecurityGroup for EC2 instance $i ...\u0026quot; aws ec2 describe-instances --instance-ids $INSTANCE_IDS | jq -r '.Reservations[].Instances[].SecurityGroups[].GroupId' done  SecurityGroup for EC2 instance i-03ea1a083c924cd78 ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-0a635aed890c7cc3e ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-048e5ec8815e5ea8a ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef  Create custom resource group1-pod-netconfig.yaml for first subnet (100.64.0.0/19). Replace the SubnetId and SecuritGroupIds with the values from above. Here is how it looks with the configuration values for my environment\nNote: We are using same SecurityGroup for pods as your Worker Nodes but you can change these and use custom SecurityGroups for your Pod Networking\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: subnet-04f960ffc8be6865c securityGroups: - sg-070d03008bda531ad - sg-06e5cab8e5d6f16ef  Create custom resource group2-pod-netconfig.yaml for second subnet (100.64.32.0/19). Replace the SubnetId and SecuritGroupIds as above.\nSimilarly, create custom resource group3-pod-netconfig.yaml for third subnet (100.64.64.0/19). Replace the SubnetId and SecuritGroupIds as above.\nCheck the instance details using this command as you will need AZ info when you apply annotation to Worker nodes using custom network config\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  Apply the CRD\u0026rsquo;s\nkubectl apply -f group1-pod-netconfig.yaml kubectl apply -f group2-pod-netconfig.yaml kubectl apply -f group3-pod-netconfig.yaml  As last step, we will annotate nodes with custom network configs.\nBe sure to annotate the instance with config that matches correct AZ. For ex, in my environment instance ip-192-168-33-135.us-east-2.compute.internal is in us-east-2b. So, I will apply group1-pod-netconfig.yaml to this instance. Similarly, I will apply group2-pod-netconfig.yaml to ip-192-168-71-211.us-east-2.compute.internal and group3-pod-netconfig.yaml to ip-192-168-9-228.us-east-2.compute.internal\n kubectl annotate node \u0026lt;nodename\u0026gt;.\u0026lt;region\u0026gt;.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  As an example, here is what I would run in my environment\nkubectl annotate node ip-192-168-33-135.us-east-2.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  You should now see secondary IP address from extended CIDR assigned to annotated nodes.\n"
},
{
	"uri": "/batch/deploy/",
	"title": "Deploy Argo",
	"tags": [],
	"description": "",
	"content": " Deploy Argo Argo run in its own namespace and deploys as a CustomResourceDefinition.\nDeploy the Controller and UI.\nkubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml  namespace/argo created customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created serviceaccount/argo created serviceaccount/argo-ui created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created clusterrole.rbac.authorization.k8s.io/argo-cluster-role created clusterrole.rbac.authorization.k8s.io/argo-ui-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-ui-binding created configmap/workflow-controller-configmap created service/argo-ui created deployment.apps/argo-ui created deployment.apps/workflow-controller created  To use advanced features of Argo for this demo, create a RoleBinding to grant admin privileges to the \u0026lsquo;default\u0026rsquo; service account.\nThis is for demo purposes only. In any other environment, you should use Workflow RBAC to set appropriate permissions.\n kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=default:default  "
},
{
	"uri": "/servicemesh_with_appmesh/create_virtual_routers/",
	"title": "Create Virtual Routers and Routes",
	"tags": [],
	"description": "",
	"content": " Virtual routers handle traffic for one or more service names within your mesh. After you create a virtual router, you can create and associate routes for your virtual router that direct incoming requests to different virtual nodes.\nNext, we\u0026rsquo;ll create two Virtual Routers.\nCreating the routers Each service name within the mesh must be fronted by a virtual router, and the service name you specify for the virtual router must be a real DNS service name within your VPC. In most cases you should just use the same service name that you specified for your virtual nodes.\nThe following JSON represents a virtual router called colorgateway-vr, for the service name colorgateway.default.svc.cluster.local.\nCopy and paste the following into your terminal to create colorgateway-vr:\naws appmesh create-virtual-router --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;serviceNames\u0026quot;: [ \u0026quot;colorgateway.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorgateway-vr\u0026quot; }'  Similarly, for the virtual node colorteller-vn, copy and paste the following into your terminal to create colorteller-vr:\naws appmesh create-virtual-router --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;serviceNames\u0026quot;: [ \u0026quot;colorteller.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  Next we\u0026rsquo;ll add routes to these virtual routers.\nCreating the Routes A route is associated with a virtual router, and it is used to match requests for a virtual router and distribute traffic accordingly to its associated virtual nodes.\nYou can use the prefix parameter in your route specification for path-based routing of requests. For example, if your virtual router service name is my-service.local, and you want the route to match requests to my-service.local/metrics, then your prefix should be /metrics.\nIf your route matches a request, you can distribute traffic to one or more target virtual nodes with relative weighting.\nThe following JSON represents a route called colorgateway-route, for the virtual router colorgateway-vr.\nThis route directs 100% of traffic to colorgateway-vn on requests matching the / prefix.\nCopy and paste the following into your terminal to create colorgateway-route:\naws appmesh create-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorgateway-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorgateway-vn\u0026quot;, \u0026quot;weight\u0026quot;: 100 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorgateway-vr\u0026quot; }'  Similarly, for the virtual router colorteller-vr, copy and paste the following into your terminal to create colorteller-route:\naws appmesh create-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-vn\u0026quot;, \u0026quot;weight\u0026quot;: 1 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  "
},
{
	"uri": "/servicemesh_with_istio/deploy/",
	"title": "Deploy Sample Apps",
	"tags": [],
	"description": "",
	"content": " Now that we have all the resources installed for Istio, we will use sample application called BookInfo to review key capabilities of the service mesh such as intelligent routing, and review telemetry data using Prometheus \u0026amp; Grafana.\nSample Apps The Bookinfo application is broken into four separate microservices:\n productpage\n The productpage microservice calls the details and reviews microservices to populate the page.  details\n The details microservice contains book information.  reviews\n The reviews microservice contains book reviews. It also calls the ratings microservice.  ratings\n The ratings microservice contains book ranking information that accompanies a book review.   There are 3 versions of the reviews microservice:\n Version v1\n doesn’t call the ratings service.  Version v2\n calls the ratings service, and displays each rating as 1 to 5 black stars.  Version v3\n calls the ratings service, and displays each rating as 1 to 5 red stars.   Deploy Sample Apps Deploy sample apps by manually injecting istio proxy and confirm pods, services are running correctly\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)  The output from\nkubectl get pod,svc  Should look similar to:\nNAME READY STATUS RESTARTS AGE details-v1-64558cf56b-dxbx2 2/2 Running 0 14s productpage-v1-5b796957dd-hqllk 2/2 Running 0 14s ratings-v1-777b98fcc4-5bfr8 2/2 Running 0 14s reviews-v1-866dcb7ff-k69jm 2/2 Running 0 14s reviews-v2-6d7959c9d-5ppnc 2/2 Running 0 14s reviews-v3-7ddf94f545-m7vls 2/2 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.100.102.153 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 138d productpage ClusterIP 10.100.222.154 \u0026lt;none\u0026gt; 9080/TCP 17s ratings ClusterIP 10.100.1.63 \u0026lt;none\u0026gt; 9080/TCP 17s reviews ClusterIP 10.100.255.157 \u0026lt;none\u0026gt; 9080/TCP 17s  Next we\u0026rsquo;ll define the virtual service and ingress gateway:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  Next, we\u0026rsquo;ll query the DNS name of the ingress gateway and use it to connect via the browser.\nkubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system ; echo  This may take a minute or two, first for the Ingress to be created, and secondly for the Ingress to hook up with the services it exposes.\nTo test, do the following:\n Open a new browser tab Paste the DNS endpoint returned from the previous get service istiogateway command Add /productpage to the end of that DNS endpoint Hit enter to retrieve the page.  Remember to add /productpage to the end of the URI in the browser to see the sample webpage!\n Click reload multiple times to see how the layout and content of the reviews changes as differnt versions (v1, v2, v3) of the app are called.\n"
},
{
	"uri": "/introduction/basics/concepts_nodes/",
	"title": "Kubernetes Nodes",
	"tags": [],
	"description": "",
	"content": "The machines that make up a Kubernetes cluster are called nodes.\nNodes in a Kubernetes cluster may be physical, or virtual.\nThere are two types of nodes:\n A Master-node type, which makes up the Control Plane, acts as the “brains” of the cluster.\n A Worker-node type, which makes up the Data Plane, runs the actual container images (via pods.)\n  We’ll dive deeper into how nodes interact with each other later in the presentation.\n"
},
{
	"uri": "/deploy/viewservices/",
	"title": "Find the Service Address",
	"tags": [],
	"description": "",
	"content": "Now that we have a running service that is type: LoadBalancer we need to find the ELB\u0026rsquo;s address. We can do this by using the get services operation of kubectl:\nkubectl get service ecsdemo-frontend  Notice the field isn\u0026rsquo;t wide enough to show the FQDN of the ELB. We can adjust the output format with this command:\nkubectl get service ecsdemo-frontend -o wide  If we wanted to use the data programatically, we can also output via json. This is an example of how we might be able to make use of json output:\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  It will take several seconds for the ELB to become healthy and start passing traffic to the frontend pods.\n You should also be able to copy/paste the loadBalancer hostname into your browser and see the application running. Keep this tab open while we scale the services up on the next page.\n"
},
{
	"uri": "/statefulset/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " First delete the StatefulSet. This will also terminates the pods. It may take some while.\nkubectl delete statefulset mysql  Verify there are no pods running by following command.\nkubectl get pods -l app=mysql  No resources found.  Delete ConfigMap, Service and PVC by following command.\nkubectl delete configmap,service,pvc -l app=mysql  configmap \u0026quot;mysql-config\u0026quot; deleted service \u0026quot;mysql\u0026quot; deleted service \u0026quot;mysql-read\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-0\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-1\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-2\u0026quot; deleted  Congratulation! You\u0026rsquo;ve finished the StatefulSets lab. "
},
{
	"uri": "/logging/configurecwl/",
	"title": "Configure CloudWatch Logs and Kibana",
	"tags": [],
	"description": "",
	"content": " Configure CloudWatch Logs Subscription CloudWatch Logs can be delivered to other services such as Amazon Elasticsearch for custom processing. This can be achieved by subscribing to a real-time feed of log events. A subscription filter defines the filter pattern to use for filtering which log events gets delivered to Elasticsearch, as well as information about where to send matching log events to.\nIn this section, we’ll subscribe to the CloudWatch log events from the fluent-cloudwatch stream from the eks/eksworkshop-eksctl log group. This feed will be streamed to the Elasticsearch cluster.\nOriginal instructions for this are available at:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html\nCreate Lambda Basic Execution Role\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/lambda.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/environment/iam_policy/lambda.json aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  Go to the CloudWatch Logs console\nSelect the log group /eks/eksworkshop-eksctl/containers. Click on Actions and select Stream to Amazon ElasticSearch Service. Select the ElasticSearch Cluster kubernetes-logs and IAM role lambda_basic_execution\nClick Next\nSelect Common Log Format and click Next\nReview the configuration. Click Next and then Start Streaming\nCloudwatch page is refreshed to show that the filter was successfully created\nConfigure Kibana In Amazon Elasticsearch console, select the kubernetes-logs under My domains\nOpen the Kibana dashboard from the link. After a few minutes, records will begin to be indexed by ElasticSearch. You\u0026rsquo;ll need to configure an index patterns in Kibana.\nSet Index Pattern as cwl-* and click Next\nSelect @timestamp from the dropdown list and select Create index pattern\nClick on Discover and explore your logs\n"
},
{
	"uri": "/scaling/test_ca/",
	"title": "Scale a Cluster with CA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an sample nginx application as a ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout  Scale our ReplicaSet OK, let\u0026rsquo;s scale out the replicaset to 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout  Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.\nkubectl get pods -o wide --watch  NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  View the cluster-autoscaler logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  You will notice Cluster Autoscaler events similar to below Check the AWS Management Console to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.\n"
},
{
	"uri": "/helm_root/helm_micro/service/",
	"title": "Test the Service",
	"tags": [],
	"description": "",
	"content": "To test the service our eksdemo Chart created, we\u0026rsquo;ll need to get the name of the ELB endpoint that was generated when we deployed the Chart:\nkubectl get svc ecsdemo-frontend -o jsonpath=\u0026quot;{.status.loadBalancer.ingress[*].hostname}\u0026quot;; echo  Copy that address, and paste it into a new tab in your browser. You should see something similar to:\n"
},
{
	"uri": "/healthchecks/",
	"title": "Health Checks",
	"tags": [],
	"description": "",
	"content": " Health Checks By default, Kubernetes will restart a container if it crashes for any reason. It uses Liveness and Readiness probes which can be configured for running a robust application by identifying the healthy containers to send traffic to and restarting the ones when required.\nIn this section, we will understand how liveness and readiness probes are defined and test the same against different states of a pod. Below is the high level description of how these probes work.\nLiveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for different reasons while Kubernetes kills and recreates the pod when liveness probe does not pass.\nReadiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes, a pod will receive traffic from the service. When readiness probe fails, traffic will not be sent to a pod until it passes.\nWe will review some examples in this module to understand different options for configuring liveness and readiness probes.\n"
},
{
	"uri": "/scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": [],
	"description": "",
	"content": " Implement AutoScaling with HPA and CA In this Chapter, we will show patterns for scaling your worker nodes and applications deployments automatically. Automatic scaling in K8s comes in two forms:\n Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).\n Cluster Autoscaler (CA) is the default K8s component that can be used to perform pod scaling as well as scaling nodes in a cluster. It automatically increases the size of an Auto Scaling group so that pods have a place to run. And it attempts to remove idle nodes, that is, nodes with no running pods.\n  "
},
{
	"uri": "/codepipeline/",
	"title": "CI/CD with CodePipeline",
	"tags": [],
	"description": "",
	"content": " CI/CD with CodePipeline Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.\nIn this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.\n"
},
{
	"uri": "/x-ray/",
	"title": "Tracing with X-Ray",
	"tags": [],
	"description": "",
	"content": " Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.\nIn this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.\n"
},
{
	"uri": "/batch/",
	"title": "Batch Processing with Argo",
	"tags": [],
	"description": "",
	"content": " Batch Processing In this Chapter, we will deploy common batch processing scenarios using Kubernetes and Argo.\nWhat is Argo? Argo is an open source container-native workflow engine for getting work done on Kubernetes. Argo is implemented as a Kubernetes CRD (Custom Resource Definition).\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo workflows on Kubernetes.  "
},
{
	"uri": "/advanced-networking/secondary_cidr/test_networking/",
	"title": "Test Networking",
	"tags": [],
	"description": "",
	"content": " Launch pods into Secondary CIDR network Let\u0026rsquo;s launch few pods and test networking\nkubectl run nginx --image=nginx kubectl scale --replicas=3 deployments/nginx kubectl expose deployment/nginx --type=NodePort --port 80 kubectl get pods -o wide  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-64f497f8fd-k962k 1/1 Running 0 40m 100.64.6.147 ip-192-168-52-113.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-lkslh 1/1 Running 0 40m 100.64.53.10 ip-192-168-74-125.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-sgz6f 1/1 Running 0 40m 100.64.80.186 ip-192-168-26-65.us-east-2.compute.internal \u0026lt;none\u0026gt;  You can use busybox pod and ping pods within same host or across hosts using IP address\nkubectl run -i --rm --tty debug --image=busybox -- sh  Test access to internet and to nginx service\n# connect to internet / # wget google.com -O - Connecting to google.com (172.217.5.238:80) Connecting to www.google.com (172.217.5.228:80) \u0026lt;!doctype html\u0026gt;\u0026lt;html itemscope=\u0026quot;\u0026quot; itemtype=\u0026quot;http://schema.org/WebPage\u0026quot; lang=\u0026quot;en\u0026quot;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta content=\u0026quot;Search the world's information, including webpages, images, videos and more. Google has many special ... # connect to service (testing core-dns) / # wget nginx -O - Connecting to nginx (10.100.170.156:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ...  "
},
{
	"uri": "/batch/artifact/",
	"title": "Configure Artifact Repository",
	"tags": [],
	"description": "",
	"content": " Configure Artifact Repository Argo uses an artifact repository to pass data between jobs in a workflow, known as artifacts. Amazon S3 can be used as an artifact repository.\nLet\u0026rsquo;s create a S3 bucket using the AWS CLI.\nACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 mb s3://batch-artifact-repository-${ACCOUNT_ID}/  Next, edit the workflow-controller ConfigMap to use the S3 bucket.\nkubectl edit -n argo configmap/workflow-controller-configmap  Add the following lines to the end of the ConfigMap, substituting your Account ID for {{ACCOUNT_ID}}:\ndata: config: | artifactRepository: s3: bucket: batch-artifact-repository-{{ACCOUNT_ID}} endpoint: s3.amazonaws.com  Create an IAM Policy In order for Argo to read from/write to the S3 bucket, we need to configure an inline policy and add it to the EC2 instance profile of the worker nodes.\nCollect the Instance Profile, Role name, and Account ID from the CloudFormation Stack.\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)  Create and policy and attach to the worker node role.\nmkdir ~/environment/batch_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/k8s-s3-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}\u0026quot;, \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}/*\u0026quot; ] } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker --policy-document file://~/environment/batch_policy/k8s-s3-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  "
},
{
	"uri": "/servicemesh_with_istio/routing/",
	"title": "Intelligent Routing",
	"tags": [],
	"description": "",
	"content": " Intelligent Routing Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, and more in a consistent manner across the services, and the application.\nBefore you can use Istio to control the Bookinfo version routing, you\u0026rsquo;ll need to define the available versions, called subsets, in destination rules.\nService versions (a.k.a. subsets) - In a continuous deployment scenario, for a given service, there can be distinct subsets of instances running different variants of the application binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Common scenarios where this occurs include A/B testing, canary rollouts, etc. The choice of a particular version can be decided based on various criterion (headers, url, etc.) and/or by weights assigned to each version. Each service has a default version consisting of all its instances.\n kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml kubectl get destinationrules -o yaml  To route to one version only, you apply virtual services that set the default version for the microservices. In this case, the virtual services will route all traffic to reviews:v1 of microservice.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1  Try now to reload the page multiple times, and note how only version 1 of reviews is displayed each time.\nNext, we\u0026rsquo;ll change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.\nkubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 in default and route v2 if the logged user is match with \u0026lsquo;jason\u0026rsquo; for reviews request.\nspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1  To test, click Sign in from the top right corner of the page, and login using jason as user name with a blank password. You will only see reviews:v2 all the time. Others will see reviews:v1.\nTo test for resiliency, inject a 7s delay between the reviews:v2 and ratings microservices for user jason. This test will uncover a bug that was intentionally introduced into the Bookinfo app.\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 in default and added 7s delay for all the request if the logged user is match with \u0026lsquo;jason\u0026rsquo; for ratings.\nspec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Logout, then click Sign in from the top right corner of the page, using jason as the user name with a blank password. You will see the delays and it ends up display error for reviews. Others will see reviews without error.\nThe timeout between the productpage and the reviews service is 6 seconds - coded as 3s + 1 retry for 6s total.\nTo test for another resiliency, introduce an HTTP abort to the ratings microservices for the test user jason. The page will immediately display the “Ratings service is currently unavailable”\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 and by default returns an error message of \u0026ldquo;Ratings service is currently unavailable\u0026rdquo; below the reviewer name if the logged username matches \u0026lsquo;jason\u0026rsquo;.\nspec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  To test, click Sign in from the top right corner of the page and login using jason for the user name with a blank password. As jason you will see the error message. Others (not logged in as jason) will see no error message.\nNext, we\u0026rsquo;ll demonstrate how to gradually migrate traffic from one version of a microservice to another. In our example, we\u0026rsquo;ll send 50% of traffic to reviews:v1 and 50% to reviews:v3.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl get virtualservice reviews -o yaml  The subset is set to 50% of traffic to v1 and 50% of traffic to v3 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50  To test it, refresh your browser over and over, and you\u0026rsquo;ll see only reviews:v1 and reviews:v3.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_k8s_app/",
	"title": "Create the k8s app",
	"tags": [],
	"description": "",
	"content": " Up to this point, we\u0026rsquo;ve created all required components of the app mesh (virtual nodes, virtual routers, and routes) to support our application. In this chapter, we\u0026rsquo;ll actually deploy the k8s application.\nDeploying the k8s Colorteller App To deploy app, copy and paste the following into your terminal:\nkubectl apply -f https://raw.githubusercontent.com/geremyCohen/colorapp/master/colorapp.yaml  Deploying Curler In addition to deploying the application, we\u0026rsquo;ll also deploy Curler, which is a simple image that provide curl functionality. To deploy the curler pods, copy and paste the following:\nkubectl run -it curler --image=tutum/curl /bin/bash  "
},
{
	"uri": "/servicemesh_with_appmesh/testing_the_app_mesh/",
	"title": "Test the App on App Mesh",
	"tags": [],
	"description": "",
	"content": " We now have both App Mesh and the application deployed. Next comes the fun part of seeing how we can use App Mesh to change the charactertics of the application.\nOpen Two Terminals You should already have one terminal open. Click the widget on the Cloud9 GUI and open a second terminal.\nUse curler to fetch the application response In the first terminal, run the following command to connect to the curler pod with a bash shell (you may be prompted to hit enter to get a command line prompt):\nkubectl run -it curler --image=tutum/curl /bin/bash  Next, with a shell open to the curler pod, paste the following to repeatedly request the colorgateway service:\nwhile [ 1 ]; do curl -s --connect-timeout 2 http://colorgateway.default.svc.cluster.local:9080/color;echo;sleep 1; done  Every second, you should see the response white.\nThis is because colorgateway always forwards to colorteller, which via the colorteller-route, always routes to colorteller-vn (which will always respond with white).\nLet\u0026rsquo;s modify the colorteller-route so it instead routes to the blue, red, and black colorteller virtual nodes, each at a 30% weighted ratio.\nModify colorteller-route To modify colorteller-route, copy and paste the following into the other terminal (the one that is not making the curl requests):\naws appmesh update-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-blue-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  If you look at the curler terminal, you should now see an equal distribution of traffic to the blue, red, and black virtual nodes.\nFor fun, to see a 50\u0026frasl;50 weighted response of only the red and black virtual nodes, copy and paste the following route:\naws appmesh update-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot;, \u0026quot;weight\u0026quot;: 5 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot;, \u0026quot;weight\u0026quot;: 5 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  "
},
{
	"uri": "/introduction/basics/concepts_objects/",
	"title": "K8s Objects Overview",
	"tags": [],
	"description": "",
	"content": "Kubernetes objects are entities that are used to represent the state of the cluster.\nAn object is a “record of intent” – once created, the cluster does its best to ensure it exists as defined. This is known as the cluster’s “desired state.”\nKubernetes is always working to make an object’s “current state” equal to the object’s “desired state.” A desired state can describe:\n What pods (containers) are running, and on which nodes IP endpoints that map to a logical group of containers How many replicas of a container are running And much more\u0026hellip;  Let’s explain these k8s objects in a bit more detail\u0026hellip;\n"
},
{
	"uri": "/deploy/scalebackend/",
	"title": "Scale the Backend Services",
	"tags": [],
	"description": "",
	"content": "When we launched our services, we only launched one container of each. We can confirm this by viewing the running pods:\nkubectl get deployments  Now let\u0026rsquo;s scale up the backend services:\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3  Confirm by looking at deployments again:\nkubectl get deployments  Also, check the browser tab where we can see our application running. You should now see traffic flowing to multiple backend services.\n"
},
{
	"uri": "/logging/cleanup/",
	"title": "Cleanup Logging",
	"tags": [],
	"description": "",
	"content": "cd ~/environment INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/fluentd/fluentd.yml rm -rf ~/environment/fluentd/ aws es delete-elasticsearch-domain --domain-name kubernetes-logs aws logs delete-log-group --log-group-name /eks/eksworkshop-eksctl/containers aws iam delete-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker aws iam detach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name lambda_basic_execution rm -rf ~/environment/iam_policy/  "
},
{
	"uri": "/scaling/cleanup/",
	"title": "Cleanup Scaling",
	"tags": [],
	"description": "",
	"content": "kubectl delete -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml rm -rf ~/environment/cluster-autoscaler aws iam delete-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker kubectl delete hpa,svc php-apache kubectl delete deployment php-apache load-generator  "
},
{
	"uri": "/helm_root/helm_micro/rolling_back/",
	"title": "Rolling Back",
	"tags": [],
	"description": "",
	"content": " Mistakes will happen during deployment, and when they do, Helm makes it easy to undo, or \u0026ldquo;roll back\u0026rdquo; to the previously deployed version.\nUpdate the demo application chart with a breaking change Open values.yaml and modify the image name under nodejs.image to brentley/ecsdemo-nodejs-non-existing. This image does not exist, so this will break our deployment.\nDeploy the updated demo application chart:\nhelm upgrade workshop ~/environment/eksdemo  The rolling upgrade will begin by creating a new nodejs pod with the new image. The new ecsdemo-nodejs Pod should fail to pull non-existing image. Run helm status command to see the ImagePullBackOff error:\nhelm status workshop  Rollback the failed upgrade Now we are going to rollback the application to the previous working release revision.\nFirst, list Helm release revisions:\nhelm history workshop  Then, rollback to the previous application revision (can rollback to any revision too):\n# rollback to the 1st revision helm rollback workshop 1  Validate workshop release status with:\nhelm status workshop  "
},
{
	"uri": "/spotworkers/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleanup our Microservices deployment\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  Cleanup the Spot Handler Daemonset\nkubectl delete -f ~/environment/spot/spot-interrupt-handler-example.yml  To clean up the worker created by this module, run the following commands:\nRemove the Worker nodes from EKS:\naws cloudformation delete-stack --stack-name \u0026quot;eksworkshop-spot-workers\u0026quot;  "
},
{
	"uri": "/cleanup/workspace/",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Since we no longer need the Cloud9 instance to have Administrator access to our account, we can delete the role we created:\n Go to the IAM Console Click Delete role in the upper right corner  Finally, let\u0026rsquo;s delete our Cloud9 EC2 Instance:\n Go to your Cloud9 Environment Select the environment named eksworkshop and pick delete  "
},
{
	"uri": "/calico/",
	"title": "Create Network Policies Using Calico",
	"tags": [],
	"description": "",
	"content": " Create Network Policies Using Calico In this Chapter, we will create some network policies using Calico and see the rules in action.\nNetwork policies allow you to define rules that determine what type of traffic is allowed to flow between different services. Using network policies you can also define rules to restrict traffic. They are a means to improve your cluster\u0026rsquo;s security.\nFor example, you can only allow traffic from frontend to backend in your application.\nNetwork policies also help in isolating traffic within namespaces. For instance, if you have separate namespaces for development and production, you can prevent traffic flow between them by restrict pod to pod communication within the same namespace.\n"
},
{
	"uri": "/logging/",
	"title": "Logging with Elasticsearch, Fluentd, and Kibana (EFK)",
	"tags": [],
	"description": "",
	"content": " Implement Logging with EFK In this Chapter, we will deploy a common Kubernetes logging pattern which consists of the following:\n Fluentd is an open source data collector providing a unified logging layer, supported by 500+ plugins connecting to many types of systems. Elasticsearch is a distributed, RESTful search and analytics engine. Kibana lets you visualize your Elasticsearch data.  Together, Fluentd, Elasticsearch and Kibana is also known as “EFK stack”. Fluentd will forward logs from the individual instances in the cluster to a centralized logging backend (CloudWatch Logs) where they are combined for higher-level reporting using ElasticSearch and Kibana.\n"
},
{
	"uri": "/monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\n"
},
{
	"uri": "/servicemesh_with_istio/",
	"title": "Service Mesh with Istio",
	"tags": [],
	"description": "",
	"content": " Service Mesh With Istio A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application.\nService mesh solutions have two distinct components that behave somewhat differently: 1) a data plane, and 2) a control plane. The following diagram illustrates the basic architecture.\n The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with Mixer, a general-purpose policy and telemetry hub.\n The control plane manages and configures the proxies to route traffic. Additionally, the control plane configures Mixers to enforce policies and collect telemetry.\n  "
},
{
	"uri": "/servicemesh_with_appmesh/",
	"title": "Service Mesh with AWS App Mesh",
	"tags": [],
	"description": "",
	"content": " Service Mesh With AWS App Mesh A service mesh is a dedicated infrastructure layer for handling service-to-service communication.\nAWS App Mesh is a service mesh based on the Envoy proxy that makes it easy to monitor and control containerized microservices. App Mesh standardizes how your microservices communicate, giving you end-to-end visibility and helping to ensure high-availability for your applications.\nApp Mesh gives you consistent visibility and network traffic controls for every microservice in an application. You can use App Mesh with Amazon ECS (using the Amazon EC2 launch type), Amazon EKS, and Kubernetes on AWS.\nApp Mesh is currently in Public Preview. In addition to this workshop module, you can visit the official AWS App Mesh page.\nThe content of this chapter was based on work found at https://github.com/awslabs/aws-app-mesh-examples. Be sure to check that repo often for the latest App Mesh demos.\n"
},
{
	"uri": "/advanced-networking/",
	"title": "Advanced VPC Networking with EKS",
	"tags": [],
	"description": "",
	"content": " Advanced VPC Networking with EKS In this Chapter, we will review some of the advanced VPC networking features with EKS.\n"
},
{
	"uri": "/statefulset/",
	"title": "Stateful containers using StatefulSets",
	"tags": [],
	"description": "",
	"content": " Stateful containers using StatefulSets StatefulSets manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods, suitable for applications that require one or more of the following.\n Stable, unique network identifiers Stable, persistent storage Ordered, graceful deployment and scaling Ordered, automated rolling updates  In this Chapter, we will review how to deploy MySQL database using StatefulSets and EBS as PersistentVolume. The example is a MySQL single master topology with multiple slaves running asynchronous replication.\n"
},
{
	"uri": "/advanced-networking/secondary_cidr/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s cleanup this tutorial\nkubectl delete deployments --all  Edit aws-node configmap and comment AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG and its value\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: #- name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG # value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Delete custom resource objects from ENIConfig CRD\nkubectl delete eniconfig/group1-pod-netconfig kubectl delete eniconfig/group2-pod-netconfig kubectl delete eniconfig/group3-pod-netconfig  Terminate EC2 instances so that fresh instances are launched with default CNI configuration\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  Delete secondary CIDR from your VPC\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') ASSOCIATION_ID=$(aws ec2 describe-vpcs --vpc-id $VPC_ID | jq -r '.Vpcs[].CidrBlockAssociationSet[] | select(.CidrBlock == \u0026quot;100.64.0.0/16\u0026quot;) | .AssociationId') aws ec2 delete-subnet --subnet-id $CGNAT_SNET1 aws ec2 delete-subnet --subnet-id $CGNAT_SNET2 aws ec2 delete-subnet --subnet-id $CGNAT_SNET3 aws ec2 disassociate-vpc-cidr-block --association-id $ASSOCIATION_ID  "
},
{
	"uri": "/batch/workflow-simple/",
	"title": "Simple Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Simple Batch Workflow Save the below manifest as \u0026lsquo;workflow-whalesay.yaml\u0026rsquo; using your favorite editor and let\u0026rsquo;s deploy the whalesay example from before using Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: whalesay- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026quot;This is an Argo Workflow!\u0026quot;]  Now deploy the workflow using the argo CLI.\nYou can also run workflow specs directly using kubectl but the argo CLI provides syntax checking, nicer output, and requires less typing. For the equivalent kubectl commands, see Argo CLI.\n argo submit --watch workflow-whalesay.yaml  Name: whalesay-2kfxb Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Started: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Finished: Sat Nov 17 10:32:16 -0500 (now) Duration: 3 seconds STEP PODNAME DURATION MESSAGE ✔ whalesay-2kfxb whalesay-2kfxb 2s  Make a note of the workflow\u0026rsquo;s name from your output (It should be similar to whalesay-xxxxx).\nConfirm the output by running the following command, substituting name of your workflow for \u0026ldquo;whalesay-xxxxx\u0026rdquo;:\nargo logs whalesay-xxxxx  ___________________________ \u0026lt; This is an Argo Workflow! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_istio/visualize/",
	"title": "Monitor &amp; Visualize",
	"tags": [],
	"description": "",
	"content": " Collecting new telemetry data Next, download a YAML file to hold configuration for the new metric and log stream that Istio will generate and collect automatically.\ncurl -LO https://eksworkshop.com/servicemesh/deploy.files/istio-telemetry.yaml kubectl apply -f istio-telemetry.yaml  Make sure Prometheus and Grafana are running\nkubectl -n istio-system get svc prometheus kubectl -n istio-system get svc grafana  Setup port-forwarding for Grafana by executing the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 \u0026amp;  Open the Istio Dashboard via the Grafana UI\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /dashboard/db/istio-mesh-dashboard  Open a new terminal tab and enter to send a traffic to the mesh\nexport SMHOST=$(kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname} ' -n istio-system) SMHOST=\u0026quot;$(echo -e \u0026quot;${SMHOST}\u0026quot; | tr -d '[:space:]')\u0026quot; while true; do curl -o /dev/null -s \u0026quot;${SMHOST}/productpage\u0026quot;; done  You will see that the traffic is evenly spread between reviews:v1 and reviews:v3\nWe encourage you to explore other Istio dashboards that are available by clicking the Istio Mesh Dashboard menu on top left of the page, and selecting a different dashboard.\n"
},
{
	"uri": "/servicemesh_with_appmesh/cleanup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": " There are three groups of components to remove when we\u0026rsquo;re done:\n App Mesh components k8s Colorteller app curler deployment  Remove App Mesh components To remove the App Mesh components, copy and paste this code into your terminal:\naws appmesh delete-route --mesh-name APP_MESH_DEMO --route-name colorteller-route --virtual-router-name colorteller-vr aws appmesh delete-route --mesh-name APP_MESH_DEMO --route-name colorgateway-route --virtual-router-name colorgateway-vr aws appmesh delete-virtual-router --mesh-name APP_MESH_DEMO --virtual-router-name colorteller-vr aws appmesh delete-virtual-router --mesh-name APP_MESH_DEMO --virtual-router-name colorgateway-vr aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorgateway-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-red-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-black-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-blue-vn aws appmesh delete-mesh --mesh-name APP_MESH_DEMO  Remove Colorteller App To remove the Colorteller App, copy and paste this code into your terminal:\nkubectl delete -f https://raw.githubusercontent.com/geremyCohen/colorapp/master/colorapp.yaml  Remove Curler To remove curler, copy and paste this code into your terminal:\nkubectl delete deployment.apps/curler  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_1/",
	"title": "K8s Objects Detail (1/2)",
	"tags": [],
	"description": "",
	"content": " Pod  A thin wrapper around one or more containers  DaemonSet  Implements a single instance of a pod on a worker node  Deployment  Details how to roll out (or roll back) across versions of your application  "
},
{
	"uri": "/deploy/scalefrontend/",
	"title": "Scale the Frontend",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s also scale our frontend service the same way:\nkubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments  Check the browser tab where we can see our application running. You should now see traffic flowing to multiple frontend services.\n"
},
{
	"uri": "/helm_root/helm_micro/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To delete the workshop release, run:\nhelm del --purge workshop  "
},
{
	"uri": "/helm_root/helm_intro/",
	"title": "Install Helm on EKS",
	"tags": [],
	"description": "",
	"content": " Install Helm on EKS Helm is a package manager and application management tool for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nHelm helps you to:\n Achieve a simple (one command) and repeatable deployment Manage application dependency, using specific versions of other application and services Manage multiple deployment configurations: test, staging, production and others Execute post/pre deployment jobs during application deployment Update/rollback and test application deployments  "
},
{
	"uri": "/conclusion/survey/",
	"title": "Let us know what you think!",
	"tags": [],
	"description": "",
	"content": " Please take our survey! (function(t,e,s,n){var o,a,c;t.SMCX=t.SMCX||[],e.getElementById(n)||(o=e.getElementsByTagName(s),a=o[o.length-1],c=e.createElement(s),c.type=\"text/javascript\",c.async=!0,c.id=n,c.src=[\"https:\"===location.protocol?\"https://\":\"http://\",\"widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgd_2BU860jlhPrsKW9DSM0aec7fijRMWQEdDb7y2zM_2FUrIx.js\"].join(\"\"),a.parentNode.insertBefore(c,a))})(window,document,\"script\",\"smcx-sdk\");Create your own user feedback survey    "
},
{
	"uri": "/calico/stars_policy_demo/",
	"title": "Stars Policy Demo",
	"tags": [],
	"description": "",
	"content": " Stars Policy Demo In this sub-chapter we create frontend, backend, client and UI services on the EKS cluster and define network policies to allow or block communication between these services. This demo also has a management UI that shows the available ingress and egress paths between each service.\n"
},
{
	"uri": "/batch/workflow-advanced/",
	"title": "Advanced Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Advanced Batch Workflow Let\u0026rsquo;s take a look at a more complex workflow, involving passing artifacts between jobs, multiple dependencies, etc.\nSave the below manifest as teardrop.yaml using your favorite editor.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: teardrop- spec: entrypoint: teardrop templates: - name: create-chain container: image: alpine:latest command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;touch /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay inputs: parameters: - name: message artifacts: - name: chain path: /tmp/message container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay-reduce inputs: parameters: - name: message artifacts: - name: chain-0 path: /tmp/message.0 - name: chain-1 path: /tmp/message.1 container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: teardrop dag: tasks: - name: create-chain template: create-chain - name: Alpha dependencies: [create-chain] template: whalesay arguments: parameters: [{name: message, value: Alpha}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Bravo dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Bravo}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Charlie dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Charlie}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Delta dependencies: [Bravo] template: whalesay arguments: parameters: [{name: message, value: Delta}] artifacts: - name: chain from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: Echo dependencies: [Bravo, Charlie] template: whalesay-reduce arguments: parameters: [{name: message, value: Echo}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Charlie.outputs.artifacts.chain}}\u0026quot; - name: Foxtrot dependencies: [Charlie] template: whalesay arguments: parameters: [{name: message, value: Foxtrot}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Golf dependencies: [Delta, Echo] template: whalesay-reduce arguments: parameters: [{name: message, value: Golf}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Delta.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: Hotel dependencies: [Echo, Foxtrot] template: whalesay-reduce arguments: parameters: [{name: message, value: Hotel}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Foxtrot.outputs.artifacts.chain}}\u0026quot;  This workflow uses a Directed Acyclic Graph (DAG) to explicitly define job dependencies. Each job in the workflow calls a whalesay template and passes a parameter with a unique name. Some jobs call a whalesay-reduce template which accepts multiple artifacts and combines them into a single artifact.\nEach job in the workflow pulls the artifact(s) and lists them in the \u0026ldquo;Chain\u0026rdquo;, then calls whalesay for the current job. Each job will then have a list of the previous job dependency chain (list of all jobs that had to complete before current job could run).\nRun the workflow.\nargo submit --watch teardrop.yaml  Name: teardrop-jfg5w Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Started: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Finished: Sat Nov 17 16:03:35 -0500 (5 minutes ago) Duration: 1 minute 53 seconds STEP PODNAME DURATION MESSAGE ✔ teardrop-jfg5w ├-✔ create-chain teardrop-jfg5w-3938249022 3s ├-✔ Alpha teardrop-jfg5w-3385521262 6s ├-✔ Bravo teardrop-jfg5w-1878939134 35s ├-✔ Charlie teardrop-jfg5w-3753534620 35s ├-✔ Foxtrot teardrop-jfg5w-2036090354 5s ├-✔ Delta teardrop-jfg5w-37094256 34s ├-✔ Echo teardrop-jfg5w-4165010455 31s ├-✔ Hotel teardrop-jfg5w-2342859904 4s └-✔ Golf teardrop-jfg5w-1687601882 30s  Continue to the Argo Dashboard to explore this model further.\n"
},
{
	"uri": "/servicemesh_with_istio/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow the below steps.\nTo remove telemetry configuration / port-forward process\nkubectl delete -f istio-telemetry.yaml  To remove the application virtual services / destination rules\nkubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl delete -f samples/bookinfo/networking/destination-rule-all.yaml  To remove the gateway / application\nkubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml  To remove Istio\nkubectl delete -f istio.yaml kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml kubectl delete namespace istio-system  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_2/",
	"title": "K8s Objects Detail (2/2)",
	"tags": [],
	"description": "",
	"content": " ReplicaSet  Ensures a defined number of pods are always running  Job  Ensures a pod properly runs to completion  Service  Maps a fixed IP address to a logical group of pods  Label  Key/Value pairs used for association and filtering  "
},
{
	"uri": "/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup "
},
{
	"uri": "/helm_root/helm_nginx/",
	"title": "Deploy Nginx With Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Nginx With Helm In this Chapter, we will dig deeper with Helm and demonstrate how to install the NGINX web server via the following steps:\n Update the Chart Repository   Search the Chart Repository   Add the Bitnami Repository   Install bitnami/nginx   Clean Up   "
},
{
	"uri": "/batch/dashboard/",
	"title": "Argo Dashboard",
	"tags": [],
	"description": "",
	"content": " Argo Dashboard Argo UI lists the workflows and visualizes each workflow (very handy for our last workflow).\nTo connect, use the same proxy connection setup in Deploy the Official Kubernetes Dashboard.\n  Show me the command   kubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n   To access the Argo Dashboard:\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/argo/services/argo-ui/proxy/  You will see the teardrop workflow from Advanced Batch Workflow. Click on it to see a visualization of the workflow.\nThe workflow should relatively look like a teardrop, and provide a live status for each job. Click on Hotel to see a summary of the Hotel job.\nThis details basic information about the job, and includes a link to the Logs. The Hotel job logs list the job dependency chain and the current whalesay, and should look similar to:\nChain: Alpha Bravo Charlie Echo Foxtrot ____________________ \u0026lt; This is Job Hotel! \u0026gt; -------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  Explore the other jobs in the workflow to see each job\u0026rsquo;s status and logs.\n"
},
{
	"uri": "/introduction/architecture/",
	"title": "Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n Architectural Overview   Control Plane   Data Plane   Kubernetes Cluster Setup   "
},
{
	"uri": "/helm_root/helm_micro/",
	"title": "Deploy Example Microservices Using Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Example Microservices Using Helm In this chapter, we will demonstrate how to deploy microservices using a custom Helm Chart, instead of doing everything manually using kubectl.\nFor detailed information on working with chart templates, refer to the Helm docs\n"
},
{
	"uri": "/batch/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup Delete all workflows argo delete --all  Remove Artifact Repository Bucket ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 rb s3://batch-artifact-repository-${ACCOUNT_ID}/ --force  Remove permissions for Artifact Repository Bucket INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws iam delete-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  Undeploy Argo kubectl delete -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml kubectl delete ns argo  Cleanup Kubernetes Job kubectl delete job/whalesay  "
},
{
	"uri": "/introduction/architecture/architecture_control_and_data_overview/",
	"title": "Architectural Overview",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n EKS Cluster Creation Workflow   What happens when you create your EKS cluster   EKS Architecture for Control plane and Worker node communication   High Level   Amazon EKS!   "
},
{
	"uri": "/deploy/cleanup/",
	"title": "Cleanup the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments:\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  "
},
{
	"uri": "/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": " Conclusion "
},
{
	"uri": "/introduction/architecture/architecture_control/",
	"title": "Control Plane",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   One or More API Servers: Entry point for REST / kubectl\n etcd: Distributed key/value store\n Controller-manager: Always evaluating current vs desired state\n Scheduler: Schedules pods to worker nodes\n  Check out the official Kubernetes documentation for a more in-depth explanation of control plane components.\n"
},
{
	"uri": "/helm_root/helm_nginx/updatecharts/",
	"title": "Update the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Helm uses a packaging format called Charts. A Chart is a collection of files that describe k8s resources.\nCharts can be simple, describing something like a standalone web server (which is what we are going to create), but they can also be more complex, for example, a chart that represents a full web application stack included web servers, databases, proxies, etc.\nInstead of installing k8s resources manually via kubectl, we can use Helm to install pre-defined Charts faster, with less chance of typos or other operator errors.\nWhen you install Helm, you are provided with a default repository of Charts from the official Helm Chart Repository.\nThis is a very dynamic list that always changes due to updates and new additions. To keep Helm\u0026rsquo;s local list updated with all these changes, we need to occasionally run the repository update command.\nTo update Helm\u0026rsquo;s local list of Charts, run:\nhelm repo update  And you should see something similar to:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. ⎈ Happy Helming!⎈  Next, we\u0026rsquo;ll search for the NGINX web server Chart.\n"
},
{
	"uri": "/introduction/architecture/architecture_worker/",
	"title": "Data Plane",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   Made up of worker nodes\n kubelet: Acts as a conduit between the API server and the node\n kube-proxy: Manages IP translation and routing\n  Check out the official Kubernetes documentation for a more in-depth explanation of data plane components.\n"
},
{
	"uri": "/introduction/architecture/cluster_setup_options/",
	"title": "Kubernetes Cluster Setup",
	"tags": [],
	"description": "",
	"content": "In addition to the managed Amazon EKS solution, there are many tools available to help bootstrap and configure a self-managed Kubernetes cluster. They include:\n Minikube – Development and Learning Kops – Learning, Development, Production Kubeadm – Learning, Development, Production Docker for Mac - Learning, Development Kubernetes IN Docker - Learning, Development  Alongside these open source solutions, there are also many commercial options available.\nLet\u0026rsquo;s take a look at Amazon EKS!\n"
},
{
	"uri": "/introduction/eks/eks_customers/",
	"title": "EKS Cluster Creation Workflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_control_plane/",
	"title": "What happens when you create your EKS cluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_architecture/",
	"title": "EKS Architecture for Control plane and Worker node communication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_level/",
	"title": "High Level",
	"tags": [],
	"description": "",
	"content": "Once your EKS cluster is ready, you get an API endpoint and you\u0026rsquo;d use Kubectl, community developed tool to interact with your cluster.\n"
},
{
	"uri": "/introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "Stay tuned as we continue the journey with EKS in the next module!\nAlways ask questions! Feel free to ask them in person during this workshop, or any time on the official Kubernetes Slack channel accessible via http://slack.k8s.io/.\n"
},
{
	"uri": "/helm_root/helm_nginx/searchchart/",
	"title": "Search the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Now that our repository Chart list has been updated, we can search for Charts.\nTo list all Charts:\nhelm search  That should output something similiar to:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.0 2.1.1 Scales worker... stable/aerospike 0.1.7 v3.14.1.2 A Helm chart... ...  You can see from the output that it dumped the list of all Charts it knows about. In some cases that may be useful, but an even more useful search would involve a keyword argument. So next, we\u0026rsquo;ll search just for NGINX:\nhelm search nginx  That results in:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress ... stable/nginx-ldapauth-proxy 0.1.2 1.13.5 nginx proxy ... stable/nginx-lego 0.3.1 Chart for... stable/gcloud-endpoints 0.1.2 1 DEPRECATED Develop... ...  This new list of Charts are specific to nginx, because we passed the nginx argument to the search command.\n"
},
{
	"uri": "/helm_root/helm_nginx/addbitnamirepo/",
	"title": "Add the Bitnami Repository",
	"tags": [],
	"description": "",
	"content": "In the last slide, we saw that NGINX offers many different products via the default Helm Chart repository, but the NGINX standalone web server is not one of them.\nAfter a quick web search, we discover that there is a Chart for the NGINX standalone web server available via the Bitnami Chart repository.\nTo add the Bitnami Chart repo to our local list of searchable charts:\nhelm repo add bitnami https://charts.bitnami.com/bitnami  Once that completes, we can search all Bitnami Charts:\nhelm search bitnami  Which results in:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.3 0.0.1 Chart with... bitnami/apache 2.1.2 2.4.37 Chart for Apache... bitnami/cassandra 0.1.0 3.11.3 Apache Cassandra... ...  Search once again for NGINX:\nhelm search nginx  Now we are seeing more NGINX options, across both repositories:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress... stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress controller ...  Or even search the Bitnami repo, just for NGINX:\nhelm search bitnami/nginx  Which narrows it down to NGINX on Bitnami:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress...  In both of those last two searches, we see\nbitnami/nginx  as a search result. That\u0026rsquo;s the one we\u0026rsquo;re looking for, so let\u0026rsquo;s use Helm to install it to the EKS cluster.\n"
},
{
	"uri": "/helm_root/helm_nginx/installnginx/",
	"title": "Install bitnami/nginx",
	"tags": [],
	"description": "",
	"content": " Installing the Bitnami standalone NGINX web server Chart involves us using the helm install command.\nWhen we install using Helm, we need to provide a deployment name, or a random one will be assigned to the deployment automatically.\nChallenge: How can you use Helm to deploy the bitnami/nginx chart?\nHINT: Use the helm utility to install the bitnami/nginx chart and specify the name mywebserver for the Kubernetes deployment. Consult the helm install documentation or run the helm install --help command to figure out the syntax\n  Expand here to see the solution   helm install --name mywebserver bitnami/nginx    Once you run this command, the output confirms the types of k8s objects that were created as a result:\nNAME: mywebserver LAST DEPLOYED: Tue Nov 13 19:55:25 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Deployment NAME AGE mywebserver-nginx 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME AGE mywebserver-nginx 0s  In the following kubectl command examples, it may take a minute or two for each of these objects\u0026rsquo; DESIRED and CURRENT values to match; if they don\u0026rsquo;t match on the first try, wait a few seconds, and run the command again to check the status.\n The first object shown in this output is a Deployment. A Deployment object manages rollouts (and rollbacks) of different versions of an application.\nYou can inspect this Deployment object in more detail by running the following command:\nkubectl describe deployment mywebserver-nginx  The next object shown created by the Chart is a Pod. A Pod is a group of one or more containers.\nTo verify the Pod object was successfully deployed, we can run the following command:\nkubectl get pods -l app=mywebserver-nginx  And you should see output similar to:\nNAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 1/1 Running 0 10s  The third object that this Chart creates for us is a Service The Service enables us to contact this NGINX web server from the Internet, via an Elastic Load Balancer (ELB).\nTo get the complete URL of this Service, run:\nkubectl get service mywebserver-nginx -o wide  That should output something similar to:\nNAME TYPE CLUSTER-IP EXTERNAL-IP mywebserver-nginx LoadBalancer 10.100.223.99 abc123.amazonaws.com  Copy the value for EXTERNAL-IP, open a new tab in your web browser, and paste it in. It may take a couple minutes for the ELB and its associated DNS name to become available; if you get an error, wait one minute, and hit reload.\n When the Service does come online, you should see a welcome message similar to:\nCongrats! You\u0026rsquo;ve now successfully deployed the NGINX standalone web server to your EKS cluster!\n"
},
{
	"uri": "/helm_root/helm_nginx/cleaningup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "To remove all the objects that the Helm Chart created, we can use Helm delete.\nBefore we delete it though, we can verify what we have running via the Helm list command:\nhelm list  You should see output similar to below, which show that mywebserver is installed:\nNAME REVISION UPDATED STATUS CHART APP VERSION mywebserver 1 Tue Nov 13 19:55:25 2018 DEPLOYED nginx-1.1.2 1.14.1  It was a lot of fun; we had some great times sending HTTP back and forth, but now its time to delete this deployment. To delete:\nhelm delete --purge mywebserver  And you should be met with the output:\nrelease \u0026quot;mywebserver\u0026quot; deleted  kubectl will also demonstrate that our pods and service are no longer available:\nkubectl get pods -l app=mywebserver-nginx kubectl get service mywebserver-nginx -o wide  As would trying to access the service via the web browser via a page reload.\nWith that, cleanup is complete.\n"
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab-with-code\").tabs();}); Tabs showing installation process:   eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab-installation\").tabs();}); Second set of tabs showing installation process:   eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more-tab-installation\").tabs();}); "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Thanks to our wonderful contributors  for making Open Source a better place! .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }  "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/example_cf_templates/",
	"title": "Example of using CloudFormation Templates",
	"tags": [],
	"description": "",
	"content": " Click below to add a CloudFormation Stack    Use these templates:       Template 1 example  Launch    Download     Template 2 example  Launch    Download     Template 3 example  Launch    Download      "
},
{
	"uri": "/prerequisites/self_paced/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": " Discover more AWS resources for building and running your application on AWS:\nMore Workshops  Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop.  Tools for AWS Fargate and Amazon ECS  Containers on AWS - Learn common best-practices for running containers on AWS fargate - Command line tool for interacting with AWS Fargate. With just a single command you can build, push, and launch your container in Fargate, orchestrated by ECS. Terraform - Use Terraform to deploy your docker containers in Fargate Wonqa is a tool for spinning up disposable QA environments in AWS Fargate, with SSL enabled by Let\u0026rsquo;s Encrypt. More details about Wonqa on the Wonder Engineering blog coldbrew - Fantastic tool that provisions ECS infrastructure, builds and deploys your container, and connects your services to an application load balancer automatically. Has a great developer experience for day to day use mu - Automates everything relating to ECS devops and CI/CD. This framework lets you write a simple metadata file and it constructs all the infrastructure you need so that you can deploy to ECS by simply pushing to your Git repo.  Courses  Microservices with Docker, Flask, and React - Learn how to build, test, and deploy microservices powered by Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/prerequisites/self_paced/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/prerequisites/self_paced/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]